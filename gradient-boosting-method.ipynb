{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Gradient Boosting is another ensemble learning technique that builds a strong learner by sequentially training weak learners, where each weak learner corrects the errors of its predecessor. Gradient Boosting is a general framework, and popular implementations include Gradient Boosting Machines (GBM) and XGBoost.\n",
    "\n",
    "**Working of Gradient Boosting:**\n",
    "\n",
    "1. Initialize Model: Initialize the model with a constant value, often the mean of the target variable.\n",
    "\n",
    "2. Compute Residuals: Compute the residuals by subtracting the predicted values from the actual target values.\n",
    "\n",
    "3. Train Weak Learner on Residuals: Train a weak learner (e.g., a shallow decision tree) on the residuals. The weak learner focuses on capturing the remaining patterns in the data not explained by the current model.\n",
    "\n",
    "4. Compute Learning Rate and Update Model: Compute the learning rate (shrinkage factor) and update the model by adding the predictions of the weak learner scaled by the learning rate. The learning rate controls the contribution of each weak learner to the final model.\n",
    "\n",
    "5. Repeat: Repeat steps 2-4 for a predefined number of iterations or until a stopping criterion is met.\n",
    "\n",
    "6. Final Prediction: The final prediction is the sum of the predictions from all weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gradient_boosting_classifier = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "gradient_boosting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gradient_boosting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-dl-1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
