{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3000fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b4c7d0d",
   "metadata": {},
   "source": [
    "#### Tensors & Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c7173b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.7551e-01,  1.0748e+00,  1.3845e+00,  6.5353e-01],\n",
      "         [-6.3221e-01,  7.9054e-01, -9.1392e-01,  7.0415e-01],\n",
      "         [ 1.3968e+00, -4.4675e-01,  2.2130e+00, -8.7841e-01]],\n",
      "\n",
      "        [[ 1.0176e-01,  8.5011e-02,  1.2939e-04,  5.2281e-01],\n",
      "         [-7.2002e-01,  3.4378e-01, -9.3164e-01,  1.1351e+00],\n",
      "         [-8.5246e-01, -1.7654e+00, -7.4132e-01, -1.9917e+00]]])\n"
     ]
    }
   ],
   "source": [
    "# tensor is multidimensional array\n",
    "import torch\n",
    "x = torch.randn(2, 3, 4)  # create a random tensor with shape (2, 3, 4)\n",
    "print(x)  # print the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0640a21b",
   "metadata": {},
   "source": [
    "#### CUDA vs MPS\n",
    "\n",
    "1. torch.cuda.is_available() checks if a CUDA-capable NVIDIA GPU is available and if PyTorch can use it. CUDA is NVIDIA’s technology for running computations on their GPUs. This is used on most Windows and Linux systems with NVIDIA GPUs.\n",
    "\n",
    "2. torch.mps.is_available() checks if Apple’s Metal Performance Shaders (MPS) backend is available. MPS is Apple’s technology for running computations on Apple Silicon (M1, M2, M3 chips) and some Intel Macs with supported GPUs. CUDA does not work on Apple Silicon; MPS is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18bfbfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on MPS: mps:0\n",
      "CUDA not available\n"
     ]
    }
   ],
   "source": [
    "if torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.randn(3, 3).to(device)\n",
    "    print(\"Running on MPS:\", x.device)\n",
    "else:\n",
    "    print(\"MPS not available\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.randn(3, 3).to(device)\n",
    "    print(\"Running on CUDA:\", x.device)\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb93b3",
   "metadata": {},
   "source": [
    "#### PyTorch Parameters (`.parameters()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca16e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.2013,  0.1039,  0.1993,  0.4580],\n",
      "        [ 0.1079, -0.2671,  0.1110, -0.2548]], requires_grad=True)\n",
      "torch.Size([2])\n",
      "Parameter containing:\n",
      "tensor([-0.0651, -0.4072], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 2)\n",
    "\n",
    "# PyTorch makes two tensors for the weights and biases.\n",
    "# Special because Pytorch marks them as things it should changes during Training.\n",
    "# When we call model.parameters(), it returns these tensors.\n",
    "model = MyModel()\n",
    "for param in model.parameters():\n",
    "    print(param.shape)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7956ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "w = nn.Parameter(torch.randn(2, 2))\n",
    "print(isinstance(w, nn.Parameter))\n",
    "\n",
    "# nn.Parameter is a special kind of tensor that is automatically registered as a parameter in the module.\n",
    "# It is used to define learnable parameters in a neural network.\n",
    "# nn.Parameter is a subclass of torch.Tensor, so it behaves like a tensor.\n",
    "\n",
    "# If you add this to a module, it will show up in .parameters()\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.my_weight = nn.Parameter(torch.randn(2, 2))\n",
    "\n",
    "model = MyModel()\n",
    "print(list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429aadd9",
   "metadata": {},
   "source": [
    "#### Self-Attention Layer (Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14bce064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3341, -0.5154, -1.2380, -0.2892, -0.4579, -0.2457],\n",
       "        [-0.6077, -0.0793,  1.2263,  0.4887, -0.1040, -0.6966],\n",
       "        [-0.2376,  0.6978, -0.2318, -0.5215,  0.0550, -0.2912],\n",
       "        [-0.1975,  0.7894, -0.4018, -0.5038,  0.0247, -0.5158],\n",
       "        [ 0.0433,  0.2650, -0.5138, -0.3914, -0.2075, -0.0951],\n",
       "        [-0.3837,  0.6075,  0.4693, -0.1916, -0.1801, -0.0152],\n",
       "        [ 0.2334, -0.6505, -1.1035, -0.1337, -0.4387, -0.4416],\n",
       "        [-0.1326,  0.3576, -0.4958, -0.5872, -0.0889, -0.1419],\n",
       "        [ 0.3356, -0.6253, -1.3418, -0.2246, -0.4712, -0.3358],\n",
       "        [-0.1198,  0.5244, -0.3412, -0.5332, -0.0867, -0.0930],\n",
       "        [-0.0488,  0.1532, -0.6066, -0.5057, -0.1759, -0.2830],\n",
       "        [-0.1535,  0.2497, -0.4927, -0.4406, -0.1209, -0.2047]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "tokens = [\"The\", \" \", \"cat\", \" \", \"sat\", \" \", \"on\", \" \", \"the\", \" \", \"mat\", \".\"]\n",
    "n_tokens = len(tokens)\n",
    "d_k = 6\n",
    "\n",
    "# randomly initialize Q, K, V with Standard Normal distribution (mean=0, std=1)\n",
    "Q = torch.randn(n_tokens, d_k) # n_tokens x d_k\n",
    "K = torch.randn(n_tokens, d_k)\n",
    "V = torch.randn(n_tokens, d_k)\n",
    "\n",
    "# (n_tokens x d_k) @ (d_k x n_tokens) = (n_tokens x n_tokens)\n",
    "scores = Q @ K.T \n",
    "\n",
    "# Values can become large, so we scale them down by the square root of d_k\n",
    "# to prevent softmax from saturating\n",
    "# scaling keeps variance of the dot product more consistent\n",
    "# (n_tokens x n_tokens) / sqrt(d_k) = (n_tokens x n_tokens)\n",
    "scaled_score = scores / (d_k ** 0.5)\n",
    "\n",
    "# softmax to get attention weights last dimension\n",
    "# For each query, softamx is applied across all keys\n",
    "# converts each row to probaility distribution\n",
    "# the last diimension corresponds to the keys\n",
    "attn_weights = F.softmax(scaled_score, dim=-1)\n",
    "\n",
    "# (n_tokens x n_tokens) @ (n_tokens x d_k) = (n_tokens x d_k)\n",
    "# the attention weights are used to weight the values\n",
    "# the result is a weighted sum of the values\n",
    "output_original = attn_weights @ V\n",
    "\n",
    "output_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbd318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
