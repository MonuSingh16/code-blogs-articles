{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3000fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b4c7d0d",
   "metadata": {},
   "source": [
    "#### Tensors & Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c7173b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.7551e-01,  1.0748e+00,  1.3845e+00,  6.5353e-01],\n",
      "         [-6.3221e-01,  7.9054e-01, -9.1392e-01,  7.0415e-01],\n",
      "         [ 1.3968e+00, -4.4675e-01,  2.2130e+00, -8.7841e-01]],\n",
      "\n",
      "        [[ 1.0176e-01,  8.5011e-02,  1.2939e-04,  5.2281e-01],\n",
      "         [-7.2002e-01,  3.4378e-01, -9.3164e-01,  1.1351e+00],\n",
      "         [-8.5246e-01, -1.7654e+00, -7.4132e-01, -1.9917e+00]]])\n"
     ]
    }
   ],
   "source": [
    "# tensor is multidimensional array\n",
    "import torch\n",
    "x = torch.randn(2, 3, 4)  # create a random tensor with shape (2, 3, 4)\n",
    "print(x)  # print the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be05306",
   "metadata": {},
   "source": [
    "##### Why do we manipulate Tensor Dimensions ?\n",
    "\n",
    "1. Batching - Models process multiple samples at once (batch dimension)\n",
    "2. Layer Requirement - Expect inputs in certain shapes\n",
    "3. Multi-head Attention - Require Splitting & merging dimensions for heads.\n",
    "4. Broadcasting - Operations like addition/multiplication may require matching shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f23dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.9935, -1.0592,  0.2853,  1.8029, -1.4281],\n",
      "          [-0.7983, -0.0774,  1.6071,  1.1049, -0.9329],\n",
      "          [-1.0563,  0.6652, -0.1686,  0.3557,  1.3713],\n",
      "          [ 0.8901, -0.2760,  0.2838, -1.6845, -0.3049]],\n",
      "\n",
      "         [[-0.0922,  0.8959, -0.5861,  0.1803, -0.7828],\n",
      "          [ 0.4372, -1.0142,  0.6472, -0.7854,  1.7937],\n",
      "          [ 0.0085,  0.8860, -1.4004,  0.2979,  1.9493],\n",
      "          [ 0.0380,  1.3593, -0.9947,  0.7738, -0.0056]],\n",
      "\n",
      "         [[ 1.7288, -0.4158,  0.0079,  1.6452, -1.0752],\n",
      "          [ 0.6869, -1.5170, -0.2340,  1.2625,  0.5970],\n",
      "          [ 1.6960,  0.7483,  0.6303, -1.0809, -1.5471],\n",
      "          [ 0.9458,  1.5987,  2.5899, -0.9784, -0.1785]]],\n",
      "\n",
      "\n",
      "        [[[-0.0931,  1.5353, -1.3917,  0.7475, -0.4839],\n",
      "          [ 0.3544,  0.4690,  0.8517, -0.5913, -0.5471],\n",
      "          [-1.3679, -0.8743,  0.6295,  0.4105,  1.5556],\n",
      "          [-0.1071,  1.1296,  1.4751, -0.1668, -0.2995]],\n",
      "\n",
      "         [[-0.2317,  0.4635,  1.0820,  0.8397,  0.0129],\n",
      "          [ 0.4140, -0.1447,  0.4233,  0.5632,  0.7187],\n",
      "          [ 1.9141,  2.0934, -2.5712, -0.2888,  0.6895],\n",
      "          [-1.4471, -0.5836,  0.1619, -0.7899,  0.7765]],\n",
      "\n",
      "         [[-0.1125,  0.4446, -1.4237, -0.0203,  1.3122],\n",
      "          [-0.2460, -0.8314, -0.1299, -0.8758, -0.3274],\n",
      "          [-0.5887,  0.1270, -0.9037,  1.1552, -0.7005],\n",
      "          [-1.0863,  0.3198, -2.6372,  0.2875, -1.1868]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4, 5)  # create a random tensor with shape (2, 3, 4, 5)\n",
    "# shape (2, 3, 4, 5) means:\n",
    "# 2 matrices, each with 3 rows and 4 columns, and \n",
    "# each element is a vector of size 5\n",
    "print(x)  # print the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035729d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0640a21b",
   "metadata": {},
   "source": [
    "#### CUDA vs MPS\n",
    "\n",
    "1. torch.cuda.is_available() checks if a CUDA-capable NVIDIA GPU is available and if PyTorch can use it. CUDA is NVIDIA’s technology for running computations on their GPUs. This is used on most Windows and Linux systems with NVIDIA GPUs.\n",
    "\n",
    "2. torch.mps.is_available() checks if Apple’s Metal Performance Shaders (MPS) backend is available. MPS is Apple’s technology for running computations on Apple Silicon (M1, M2, M3 chips) and some Intel Macs with supported GPUs. CUDA does not work on Apple Silicon; MPS is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18bfbfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on MPS: mps:0\n",
      "CUDA not available\n"
     ]
    }
   ],
   "source": [
    "if torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.randn(3, 3).to(device)\n",
    "    print(\"Running on MPS:\", x.device)\n",
    "else:\n",
    "    print(\"MPS not available\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.randn(3, 3).to(device)\n",
    "    print(\"Running on CUDA:\", x.device)\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb93b3",
   "metadata": {},
   "source": [
    "#### PyTorch Parameters (`.parameters()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca16e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.2013,  0.1039,  0.1993,  0.4580],\n",
      "        [ 0.1079, -0.2671,  0.1110, -0.2548]], requires_grad=True)\n",
      "torch.Size([2])\n",
      "Parameter containing:\n",
      "tensor([-0.0651, -0.4072], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 2)\n",
    "\n",
    "# PyTorch makes two tensors for the weights and biases.\n",
    "# Special because Pytorch marks them as things it should changes during Training.\n",
    "# When we call model.parameters(), it returns these tensors.\n",
    "model = MyModel()\n",
    "for param in model.parameters():\n",
    "    print(param.shape)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7956ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "w = nn.Parameter(torch.randn(2, 2))\n",
    "print(isinstance(w, nn.Parameter))\n",
    "\n",
    "# nn.Parameter is a special kind of tensor that is automatically registered as a parameter in the module.\n",
    "# It is used to define learnable parameters in a neural network.\n",
    "# nn.Parameter is a subclass of torch.Tensor, so it behaves like a tensor.\n",
    "\n",
    "# If you add this to a module, it will show up in .parameters()\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.my_weight = nn.Parameter(torch.randn(2, 2))\n",
    "\n",
    "model = MyModel()\n",
    "print(list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429aadd9",
   "metadata": {},
   "source": [
    "#### Self-Attention Layer (Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14bce064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3341, -0.5154, -1.2380, -0.2892, -0.4579, -0.2457],\n",
       "        [-0.6077, -0.0793,  1.2263,  0.4887, -0.1040, -0.6966],\n",
       "        [-0.2376,  0.6978, -0.2318, -0.5215,  0.0550, -0.2912],\n",
       "        [-0.1975,  0.7894, -0.4018, -0.5038,  0.0247, -0.5158],\n",
       "        [ 0.0433,  0.2650, -0.5138, -0.3914, -0.2075, -0.0951],\n",
       "        [-0.3837,  0.6075,  0.4693, -0.1916, -0.1801, -0.0152],\n",
       "        [ 0.2334, -0.6505, -1.1035, -0.1337, -0.4387, -0.4416],\n",
       "        [-0.1326,  0.3576, -0.4958, -0.5872, -0.0889, -0.1419],\n",
       "        [ 0.3356, -0.6253, -1.3418, -0.2246, -0.4712, -0.3358],\n",
       "        [-0.1198,  0.5244, -0.3412, -0.5332, -0.0867, -0.0930],\n",
       "        [-0.0488,  0.1532, -0.6066, -0.5057, -0.1759, -0.2830],\n",
       "        [-0.1535,  0.2497, -0.4927, -0.4406, -0.1209, -0.2047]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "tokens = [\"The\", \" \", \"cat\", \" \", \"sat\", \" \", \"on\", \" \", \"the\", \" \", \"mat\", \".\"]\n",
    "n_tokens = len(tokens)\n",
    "d_k = 6\n",
    "\n",
    "# randomly initialize Q, K, V with Standard Normal distribution (mean=0, std=1)\n",
    "Q = torch.randn(n_tokens, d_k) # n_tokens x d_k\n",
    "K = torch.randn(n_tokens, d_k)\n",
    "V = torch.randn(n_tokens, d_k)\n",
    "\n",
    "# (n_tokens x d_k) @ (d_k x n_tokens) = (n_tokens x n_tokens)\n",
    "scores = Q @ K.T \n",
    "\n",
    "# Values can become large, so we scale them down by the square root of d_k\n",
    "# to prevent softmax from saturating\n",
    "# scaling keeps variance of the dot product more consistent\n",
    "# (n_tokens x n_tokens) / sqrt(d_k) = (n_tokens x n_tokens)\n",
    "scaled_score = scores / (d_k ** 0.5)\n",
    "\n",
    "# softmax to get attention weights last dimension\n",
    "# For each query, softamx is applied across all keys\n",
    "# converts each row to probaility distribution\n",
    "# the last diimension corresponds to the keys\n",
    "attn_weights = F.softmax(scaled_score, dim=-1)\n",
    "\n",
    "# (n_tokens x n_tokens) @ (n_tokens x d_k) = (n_tokens x d_k)\n",
    "# the attention weights are used to weight the values\n",
    "# the result is a weighted sum of the values\n",
    "output_original = attn_weights @ V\n",
    "\n",
    "output_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfbd318",
   "metadata": {},
   "source": [
    "##### PyTorch Modules & Containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50194e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Module holds a list of layers, each is a linear layer\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, output_dim) for _ in range(num_layers)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Iterate through each layer in ModuleList\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# ModuleList register each layer as a submoudle,\n",
    "# so their parameters are included in model.parameters() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c95dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=30, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 30)\n",
    ")\n",
    "\n",
    "# Sequential - to define a model as a sequence of layers.\n",
    "# It is a subclass of nn.Module that allows you to stack layers in a sequential manner.\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1becd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictModel(\n",
      "  (layers): ModuleDict(\n",
      "    (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (fc2): Linear(in_features=20, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DictModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ModuleDict holds named layers\n",
    "        self.layers = nn.ModuleDict(\n",
    "            {\n",
    "                \"fc1\": nn.Linear(10, 20),\n",
    "                'fc2': nn.Linear(20, 5)\n",
    "            }\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layers['fc1'](x)\n",
    "        x = self.layers['fc2'](x)\n",
    "        return x\n",
    "\n",
    "# ModuleDict is useful when you want to access layers by name\n",
    "model = DictModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2785be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871bd07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb3cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
