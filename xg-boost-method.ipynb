{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost\n",
    "\n",
    "XGBoost is an efficient and scalable implementation of the gradient boosting framework. It is widely used for both classification and regression tasks in machine learning. XGBoost is known for its speed, performance, and regularization techniques, making it a popular choice in various data science competitions.\n",
    "\n",
    "**Working of XGBoost:**\n",
    "\n",
    "1. Initialize Model: Start with an initial model, often the mean of the target variable.\n",
    "\n",
    "2. Compute Residuals: Compute the residuals by subtracting the predicted values from the actual target values.\n",
    "\n",
    "3. Train Weak Learner on Negative Gradient: Train a weak learner (decision tree) on the negative gradient of the loss function. The negative gradient points in the direction of steepest decrease in the loss, so the weak learner is fit to correct the mistakes of the previous model.\n",
    "\n",
    "4. Compute Learning Rate and Update Model: Compute the learning rate (shrinkage factor) and update the model by adding the predictions of the weak learner scaled by the learning rate. The learning rate controls the contribution of each weak learner to the final model.\n",
    "\n",
    "5. Regularization: Apply regularization techniques such as tree pruning and feature importance to control overfitting and improve generalization.\n",
    "\n",
    "6. Repeat: Repeat steps 2-5 for a predefined number of iterations or until a stopping criterion is met.\n",
    "\n",
    "7. Final Prediction: The final prediction is the sum of the predictions from all weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Explanation:**\n",
    "\n",
    "1. XGBoost builds a series of decision trees sequentially.\n",
    "2. Each tree corrects the errors of the previous one.\n",
    "3. Trees are trained on the negative gradient of the loss function.\n",
    "4. Regularization techniques are used to prevent overfitting.\n",
    "5. The final prediction is the sum of predictions from all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-dl-1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
