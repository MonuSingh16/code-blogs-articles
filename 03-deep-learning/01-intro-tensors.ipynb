{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b4c7d0d",
   "metadata": {},
   "source": [
    "#### Tensors & Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c7173b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.7551e-01,  1.0748e+00,  1.3845e+00,  6.5353e-01],\n",
      "         [-6.3221e-01,  7.9054e-01, -9.1392e-01,  7.0415e-01],\n",
      "         [ 1.3968e+00, -4.4675e-01,  2.2130e+00, -8.7841e-01]],\n",
      "\n",
      "        [[ 1.0176e-01,  8.5011e-02,  1.2939e-04,  5.2281e-01],\n",
      "         [-7.2002e-01,  3.4378e-01, -9.3164e-01,  1.1351e+00],\n",
      "         [-8.5246e-01, -1.7654e+00, -7.4132e-01, -1.9917e+00]]])\n"
     ]
    }
   ],
   "source": [
    "# tensor is multidimensional array\n",
    "import torch\n",
    "x = torch.randn(2, 3, 4)  # create a random tensor with shape (2, 3, 4)\n",
    "print(x)  # print the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be05306",
   "metadata": {},
   "source": [
    "##### Why do we manipulate Tensor Dimensions ?\n",
    "\n",
    "1. Batching - Models process multiple samples at once (batch dimension)\n",
    "2. Layer Requirement - Expect inputs in certain shapes\n",
    "3. Multi-head Attention - Require Splitting & merging dimensions for heads.\n",
    "4. Broadcasting - Operations like addition/multiplication may require matching shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f23dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([3, 2, 4])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)  # create a random tensor with shape (2, 3, 4, 5)\n",
    "# shape (2, 3, 4, 5) means:\n",
    "# 2 matrices, each with 3 rows and 4 columns, and \n",
    "# each element is a vector of size 5\n",
    "print(x.shape)  # print the tensor\n",
    "y = x.transpose(0, 1) # transpose the first two dimensions\n",
    "# Switching betweeen the [batch, seuence, feature] and [sequence, batch, feature] formats\n",
    "print(y.shape)  # print the transposed tensor\n",
    "\n",
    "# Again back to [batch, sequence, feature]\n",
    "y = y.permute(1, 0, 2)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping / view\n",
    "# Changes shape of tensor without changing its data.\n",
    "# Used to flatten images, prepare batches,\n",
    "\n",
    "x = torch.arange(6) # Shape [6]\n",
    "x_reshaped = x.view(2, 3) # Shape [2, 3]\n",
    "\n",
    "# Use -1 to let library infer correct dimension. x.view(-1, 3)\n",
    "\n",
    "# 1. Flattening for Fully Connected Layers\n",
    "# linear layer expects [batch, features], not [batch, channesl, height, width]\n",
    "x = torch.randn(32, 3, 28, 28) # [batch, channesl, height, width]\n",
    "x_flat = x.view(32, -1) # [batch, features]\n",
    "\n",
    "# 2. Adding a Batch Dimension\n",
    "# if you have single sample but model expect a batch\n",
    "x = torch.randn(10) # [features]\n",
    "x_batch = x.unsqueeze(0) # [1, features]\n",
    "\n",
    "# 3. Preparing Sequnces for RNNs\n",
    "# PyTorch RNNs expect [seq, batch, features]\n",
    "x = torch.randn(64, 10, 128) # [batch, seq, features]\n",
    "x_seq_first = x.permute(1, 0, 2) # [seq, batch, features]\n",
    "\n",
    "# 4. unsqueeze - adds a new dim of size 1 at specified position (axis)\n",
    "# enables broadcasting, expected input shape of a layer\n",
    "x = torch.tensor([1, 2, 3]) # Shape [3]\n",
    "x1 = x.unsqueeze(0) # shape: [1, 3]\n",
    "x2 = x.unsqueeze(1) # shape: [3, 1]\n",
    "\n",
    "# Suppose model expects [batch, features], but you have a single feature\n",
    "\n",
    "# 5. squeeze - removes all dim of size\n",
    "# reduce rank of tensor oprations\n",
    "y = torch.rann(1, 3, 1, 5)\n",
    "y1 = y.squeeze() # Shape: [3, 5], (removes all size-1 dimes)\n",
    "y2 = y.squeeze(2) # Shape: [1, 3, 5] (removes only dim 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e26b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. cat - joins a squence of tensor along an existing dims\n",
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(2, 3)\n",
    "\n",
    "cat0 = torch.cat([a, b], dim=0) # concatenate along rows (dim=0): shape [4, 3]\n",
    "cate1 = torch.cat([a, b], dim=1) # along columns (dim=1): shape [2, 6]\n",
    "\n",
    "# 7. stack - Squence of tensor along a new dim\n",
    "stack0 = torch.stack([a, b], dim=0) # row, result shape [2, 2, 3]\n",
    "stack1 = torch.stack([a, b], dim=1) # column, shape[2, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9547fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. split - divide a tensor into a list of smaller tensor of specified size(s)\n",
    "# useful for dividing data into mini-bath, splitting features\n",
    "x = torch.arnage(12).reshape(3, 4) # Shape [3, 4]\n",
    "splits = torch.split(x, 2, dim=1) # column, split into 2. # [3, 2] [3, 2]\n",
    "\n",
    "# 9. chunnk - divide into specified number of equal chunks\n",
    "chunks = torch.chunk(x, 2, dim=0) # 2 parts along rows. #[2, 4] [1, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018cd2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Reduction Operations (sum, mean, max, min)\n",
    "# collapse one or more dims. Specify the dim to reduce\n",
    "x = torch.tensor([[1., 2.], [3., 4.]])\n",
    "total = x.sum()\n",
    "row_sum = x.sum(dim=0) # [4., 6.]\n",
    "col_mean = x.mean(dim=1) #[1.5, 3.5]\n",
    "max_val, max_idx = x.max(dim=1) #([2., 4.], [1, 1]) # max val & its index\n",
    "\n",
    "# loss calculation, loss = (pred - target).pow(2).mean()\n",
    "# pooling layers: torch.maxpool2d\n",
    "# Normalization: x - x.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91efd940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Matrix Multiplications, Dot Prodction\n",
    "# Use @ or torch.matmul for matrix multiplication\n",
    "# Use torch.dot for 1D vectors\n",
    "# Linear Layer = x @ W.T + b\n",
    "# Attention Score = Q @ K.T\n",
    "\n",
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(3, 4)\n",
    "\n",
    "# Matrix Multiplication: [2, 3] @ [3, 4] -> [2, 4]\n",
    "c = a @ b\n",
    "\n",
    "# Dot Prodcut : [3], [3] -> Scalar\n",
    "v1= torch.tensor([1., 2., 3.])\n",
    "v2 = torch.tensor([4., 5., 6.])\n",
    "dot = torch.dot(v1, v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035729d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0640a21b",
   "metadata": {},
   "source": [
    "#### CUDA vs MPS\n",
    "\n",
    "1. torch.cuda.is_available() checks if a CUDA-capable NVIDIA GPU is available and if PyTorch can use it. CUDA is NVIDIA’s technology for running computations on their GPUs. This is used on most Windows and Linux systems with NVIDIA GPUs.\n",
    "\n",
    "2. torch.mps.is_available() checks if Apple’s Metal Performance Shaders (MPS) backend is available. MPS is Apple’s technology for running computations on Apple Silicon (M1, M2, M3 chips) and some Intel Macs with supported GPUs. CUDA does not work on Apple Silicon; MPS is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18bfbfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on MPS: mps:0\n",
      "CUDA not available\n"
     ]
    }
   ],
   "source": [
    "if torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.randn(3, 3).to(device)\n",
    "    print(\"Running on MPS:\", x.device)\n",
    "else:\n",
    "    print(\"MPS not available\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.randn(3, 3).to(device)\n",
    "    print(\"Running on CUDA:\", x.device)\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb93b3",
   "metadata": {},
   "source": [
    "#### PyTorch Parameters (`.parameters()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca16e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.2013,  0.1039,  0.1993,  0.4580],\n",
      "        [ 0.1079, -0.2671,  0.1110, -0.2548]], requires_grad=True)\n",
      "torch.Size([2])\n",
      "Parameter containing:\n",
      "tensor([-0.0651, -0.4072], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 2)\n",
    "\n",
    "# PyTorch makes two tensors for the weights and biases.\n",
    "# Special because Pytorch marks them as things it should changes during Training.\n",
    "# When we call model.parameters(), it returns these tensors.\n",
    "model = MyModel()\n",
    "for param in model.parameters():\n",
    "    print(param.shape)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7956ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "w = nn.Parameter(torch.randn(2, 2))\n",
    "print(isinstance(w, nn.Parameter))\n",
    "\n",
    "# nn.Parameter is a special kind of tensor that is automatically registered as a parameter in the module.\n",
    "# It is used to define learnable parameters in a neural network.\n",
    "# nn.Parameter is a subclass of torch.Tensor, so it behaves like a tensor.\n",
    "\n",
    "# If you add this to a module, it will show up in .parameters()\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.my_weight = nn.Parameter(torch.randn(2, 2))\n",
    "\n",
    "model = MyModel()\n",
    "print(list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429aadd9",
   "metadata": {},
   "source": [
    "#### Self-Attention Layer (Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14bce064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3341, -0.5154, -1.2380, -0.2892, -0.4579, -0.2457],\n",
       "        [-0.6077, -0.0793,  1.2263,  0.4887, -0.1040, -0.6966],\n",
       "        [-0.2376,  0.6978, -0.2318, -0.5215,  0.0550, -0.2912],\n",
       "        [-0.1975,  0.7894, -0.4018, -0.5038,  0.0247, -0.5158],\n",
       "        [ 0.0433,  0.2650, -0.5138, -0.3914, -0.2075, -0.0951],\n",
       "        [-0.3837,  0.6075,  0.4693, -0.1916, -0.1801, -0.0152],\n",
       "        [ 0.2334, -0.6505, -1.1035, -0.1337, -0.4387, -0.4416],\n",
       "        [-0.1326,  0.3576, -0.4958, -0.5872, -0.0889, -0.1419],\n",
       "        [ 0.3356, -0.6253, -1.3418, -0.2246, -0.4712, -0.3358],\n",
       "        [-0.1198,  0.5244, -0.3412, -0.5332, -0.0867, -0.0930],\n",
       "        [-0.0488,  0.1532, -0.6066, -0.5057, -0.1759, -0.2830],\n",
       "        [-0.1535,  0.2497, -0.4927, -0.4406, -0.1209, -0.2047]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "tokens = [\"The\", \" \", \"cat\", \" \", \"sat\", \" \", \"on\", \" \", \"the\", \" \", \"mat\", \".\"]\n",
    "n_tokens = len(tokens)\n",
    "d_k = 6\n",
    "\n",
    "# randomly initialize Q, K, V with Standard Normal distribution (mean=0, std=1)\n",
    "Q = torch.randn(n_tokens, d_k) # n_tokens x d_k\n",
    "K = torch.randn(n_tokens, d_k)\n",
    "V = torch.randn(n_tokens, d_k)\n",
    "\n",
    "# (n_tokens x d_k) @ (d_k x n_tokens) = (n_tokens x n_tokens)\n",
    "scores = Q @ K.T \n",
    "\n",
    "# Values can become large, so we scale them down by the square root of d_k\n",
    "# to prevent softmax from saturating\n",
    "# scaling keeps variance of the dot product more consistent\n",
    "# (n_tokens x n_tokens) / sqrt(d_k) = (n_tokens x n_tokens)\n",
    "scaled_score = scores / (d_k ** 0.5)\n",
    "\n",
    "# softmax to get attention weights last dimension\n",
    "# For each query, softamx is applied across all keys\n",
    "# converts each row to probaility distribution\n",
    "# the last diimension corresponds to the keys\n",
    "attn_weights = F.softmax(scaled_score, dim=-1)\n",
    "\n",
    "# (n_tokens x n_tokens) @ (n_tokens x d_k) = (n_tokens x d_k)\n",
    "# the attention weights are used to weight the values\n",
    "# the result is a weighted sum of the values\n",
    "output_original = attn_weights @ V\n",
    "\n",
    "output_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfbd318",
   "metadata": {},
   "source": [
    "##### PyTorch Modules & Containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50194e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Module holds a list of layers, each is a linear layer\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, output_dim) for _ in range(num_layers)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Iterate through each layer in ModuleList\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# ModuleList register each layer as a submoudle,\n",
    "# so their parameters are included in model.parameters() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c95dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=30, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 30)\n",
    ")\n",
    "\n",
    "# Sequential - to define a model as a sequence of layers.\n",
    "# It is a subclass of nn.Module that allows you to stack layers in a sequential manner.\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1becd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictModel(\n",
      "  (layers): ModuleDict(\n",
      "    (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (fc2): Linear(in_features=20, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DictModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ModuleDict holds named layers\n",
    "        self.layers = nn.ModuleDict(\n",
    "            {\n",
    "                \"fc1\": nn.Linear(10, 20),\n",
    "                'fc2': nn.Linear(20, 5)\n",
    "            }\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layers['fc1'](x)\n",
    "        x = self.layers['fc2'](x)\n",
    "        return x\n",
    "\n",
    "# ModuleDict is useful when you want to access layers by name\n",
    "model = DictModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb3cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-blogs-articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
