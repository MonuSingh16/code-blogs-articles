{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096622fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monusingh/work-share/code-blogs-articles/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which countries does the Amazon River flow through?\n",
      "Answer: Peru,\n",
      "Colombia, and Brazil\n",
      "Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained question-answering model, e.g., BERT fine-tuned on SQuAD\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "context = \"\"\"\n",
    "The Amazon River is the largest river by discharge volume of water in the world,\n",
    "and by some definitions, it is the longest. It flows through Peru,\n",
    "Colombia, and Brazil before emptying into the Atlantic Ocean.\n",
    "\"\"\"\n",
    "question = \"Which countries does the Amazon River flow through?\"\n",
    "\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Score: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fbc2062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Tokenizer and Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and base model loaded.\n",
      "\n",
      "--- Step 2: Input Text ---\n",
      "Original Text: 'This movie was absolutely fantastic! I loved every minute of it.'\n",
      "\n",
      "--- Step 3: Tokenization ---\n",
      "Tokenized Input IDs: tensor([[  101,  2023,  3185,  2001,  7078, 10392,   999,  1045,  3866,  2296,\n",
      "          3371,  1997,  2009,  1012,   102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Decoded Tokens: ['[CLS]', 'this', 'movie', 'was', 'absolutely', 'fantastic', '!', 'i', 'loved', 'every', 'minute', 'of', 'it', '.', '[SEP]']\n",
      "\n",
      "--- Step 4: Model Forward Pass & Logits ---\n",
      "Raw Logits from model: tensor([[-0.0541,  0.0460]], grad_fn=<AddmmBackward0>)\n",
      "Shape of Last Layer Contextual Embeddings: torch.Size([1, 15, 768])\n",
      "Shape of [CLS] Token Embedding: torch.Size([1, 768])\n",
      "\n",
      "--- Step 5: Probabilities and Prediction ---\n",
      "Probabilities (e.g., for 2 classes): tensor([[0.4750, 0.5250]], grad_fn=<SoftmaxBackward0>)\n",
      "Predicted Class Index: 1\n",
      "\n",
      "--- Step 6: Using a Dedicated Sentiment Analysis Pipeline ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: finiteautomata/bertweet-base-sentiment-analysis\n",
      "Sentence: 'This movie was absolutely fantastic! I loved every minute of it.'\n",
      "Sentiment Analysis Result: [{'label': 'POS', 'score': 0.9918115139007568}]\n",
      "\n",
      "Sentence: 'This product broke after one day, completely useless.'\n",
      "Sentiment Analysis Result: [{'label': 'NEG', 'score': 0.9826966524124146}]\n",
      "\n",
      "--- Sentiment Analysis Process Complete ---\n",
      "Key takeaway: Pre-trained embeddings provide meaningful numerical representations that a classification head then uses to predict sentiment.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, pipeline\n",
    "\n",
    "# --- Step 1: Load Pre-trained Tokenizer and Model ---\n",
    "# The tokenizer is responsible for converting raw text into numerical IDs that the model understands.\n",
    "# It also handles special tokens (like [CLS], [SEP]) and subword tokenization.\n",
    "print(\"--- Step 1: Loading Tokenizer and Model ---\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# DistilBertForSequenceClassification is a DistilBERT model with a classification head\n",
    "# (a linear layer) on top, specifically designed for tasks like sentiment analysis.\n",
    "# This model has been pre-trained on a massive text corpus and then\n",
    "# fine-tuned on a sentiment dataset (implicitly, by using a model often associated with sentiment).\n",
    "# For a raw pre-trained model that hasn't been fine-tuned for classification yet,\n",
    "# you would typically use `DistilBertModel` and add your own classification head.\n",
    "# Here, we're simulating a common scenario where a pre-trained model *is* the sentiment analyzer.\n",
    "# We're loading a general DistilBERT, then we'll show how its outputs are used for classification.\n",
    "# For a truly 'ready-to-go' sentiment model, you'd load one specifically fine-tuned for it.\n",
    "# Let's load the *base* model first to show the embedding process.\n",
    "# We will later use a `pipeline` for an end-to-end solution.\n",
    "model_base = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "print(\"Tokenizer and base model loaded.\")\n",
    "\n",
    "# --- Step 2: Prepare Input Text ---\n",
    "# We'll use a sample sentence for sentiment analysis.\n",
    "text = \"This movie was absolutely fantastic! I loved every minute of it.\"\n",
    "print(f\"\\n--- Step 2: Input Text ---\")\n",
    "print(f\"Original Text: '{text}'\")\n",
    "\n",
    "# --- Step 3: Tokenize the Input Text ---\n",
    "# The tokenizer converts the text into a sequence of numerical IDs (input_ids),\n",
    "# and also generates an attention mask.\n",
    "# `input_ids`: numerical representation of tokens.\n",
    "# `attention_mask`: indicates which tokens are real words (1) and which are padding (0).\n",
    "# `return_tensors='pt'` ensures the output is a PyTorch tensor.\n",
    "print(\"\\n--- Step 3: Tokenization ---\")\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "print(f\"Tokenized Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Attention Mask: {inputs['attention_mask']}\")\n",
    "print(f\"Decoded Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "# Note: [CLS] is a special token at the beginning for classification tasks.\n",
    "# [SEP] is a special token to separate sentences. DistilBERT does not explicitly use [SEP] for single sentences,\n",
    "# but it's common in BERT-like models for pair tasks.\n",
    "\n",
    "# --- Step 4: Obtain Embeddings (and logits) from the Pre-trained Model ---\n",
    "# When you pass the tokenized inputs to the DistilBertForSequenceClassification model,\n",
    "# it first generates contextual embeddings for each token.\n",
    "# Then, a classification head (a dense layer) on top of these embeddings\n",
    "# produces logits (raw scores) for each possible class (e.g., positive, negative).\n",
    "print(\"\\n--- Step 4: Model Forward Pass & Logits ---\")\n",
    "# When `output_hidden_states=True` is passed, the model will also return\n",
    "# the hidden states (which include the embeddings at different layers).\n",
    "outputs = model_base(**inputs, output_hidden_states=True)\n",
    "\n",
    "# The `logits` are the raw, unnormalized scores for each class.\n",
    "# For binary sentiment (positive/negative), there would be 2 logits.\n",
    "# For 3 classes (positive/negative/neutral), there would be 3 logits.\n",
    "# By default, distilbert-base-uncased is not fine-tuned for classification,\n",
    "# so it just has a default number of classes (usually 2, as it's often set up for binary tasks).\n",
    "logits = outputs.logits\n",
    "print(f\"Raw Logits from model: {logits}\") # e.g., tensor([[-0.2467,  0.1362]])\n",
    "\n",
    "# The `hidden_states` contain the embeddings from all layers.\n",
    "# The last hidden state is often used for downstream tasks.\n",
    "# `hidden_states[0]` is the embedding layer output (input embeddings before any transformer layers).\n",
    "# `hidden_states[-1]` is the output of the *last* Transformer layer (contextualized embeddings).\n",
    "# The shape will be (batch_size, sequence_length, hidden_size).\n",
    "# For `distilbert-base-uncased`, hidden_size is 768.\n",
    "last_hidden_state_embeddings = outputs.hidden_states[-1]\n",
    "print(f\"Shape of Last Layer Contextual Embeddings: {last_hidden_state_embeddings.shape}\")\n",
    "# Example: torch.Size([1, 12, 768]) for our sample sentence.\n",
    "# 1 (batch size), 12 (number of tokens), 768 (embedding dimension)\n",
    "\n",
    "# To get the embedding for the [CLS] token (often used for classification):\n",
    "cls_embedding = last_hidden_state_embeddings[:, 0, :]\n",
    "print(f\"Shape of [CLS] Token Embedding: {cls_embedding.shape}\")\n",
    "# Example: torch.Size([1, 768]) - this single vector represents the entire sentence's context\n",
    "# and is passed to the classification head.\n",
    "\n",
    "# --- Step 5: Convert Logits to Probabilities and Predict Sentiment ---\n",
    "# We apply a softmax function to the logits to get probabilities across classes.\n",
    "print(\"\\n--- Step 5: Probabilities and Prediction ---\")\n",
    "probabilities = torch.softmax(logits, dim=1)\n",
    "print(f\"Probabilities (e.g., for 2 classes): {probabilities}\")\n",
    "\n",
    "# To get the predicted class index (0 or 1 for binary classification)\n",
    "predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
    "print(f\"Predicted Class Index: {predicted_class_idx}\")\n",
    "\n",
    "# Note: Without knowing the specific fine-tuning mapping (e.g., 0=negative, 1=positive),\n",
    "# these indices are abstract for `model_base`.\n",
    "# For real sentiment analysis, you'd use a model already fine-tuned with labels.\n",
    "\n",
    "# --- Step 6: Using a Pre-trained Sentiment Analysis Pipeline (End-to-End) ---\n",
    "# For practical sentiment analysis, you'd typically use a model already fine-tuned for the task.\n",
    "# Hugging Face `pipeline` abstracts away much of the above.\n",
    "print(\"\\n--- Step 6: Using a Dedicated Sentiment Analysis Pipeline ---\")\n",
    "# This loads a model already fine-tuned for sentiment, complete with appropriate labels.\n",
    "# E.g., 'nlptown/bert-base-multilingual-uncased-sentiment' is a common one for 5-star sentiment.\n",
    "# Let's use a simpler one if available, or just demonstrate the concept.\n",
    "# A common choice for English sentiment is 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "# but it's better to use one that clearly outputs \"positive\" or \"negative\".\n",
    "# For simplicity, let's use a popular readily available one.\n",
    "try:\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "    print(f\"Model loaded: finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "\n",
    "    result = sentiment_pipeline(text)\n",
    "    print(f\"Sentence: '{text}'\")\n",
    "    print(f\"Sentiment Analysis Result: {result}\")\n",
    "    # Example Output: [{'label': 'POS', 'score': 0.9989}]\n",
    "\n",
    "    text_negative = \"This product broke after one day, completely useless.\"\n",
    "    result_negative = sentiment_pipeline(text_negative)\n",
    "    print(f\"\\nSentence: '{text_negative}'\")\n",
    "    print(f\"Sentiment Analysis Result: {result_negative}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not load specific sentiment model: {e}\")\n",
    "    print(\"Falling back to a general example or skipping pipeline demo.\")\n",
    "    print(\"The key takeaway is that the fine-tuned model leverages the pre-trained embeddings.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Sentiment Analysis Process Complete ---\")\n",
    "print(\"Key takeaway: Pre-trained embeddings provide meaningful numerical representations that a classification head then uses to predict sentiment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-blogs-articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
