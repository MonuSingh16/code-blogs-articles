{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68b5ef4",
   "metadata": {},
   "source": [
    "#### LoRA\n",
    "\n",
    "LoRA, or Low-Rank Adaptation, is a technique designed for fine-tuning large models efficiently by only updating a small set of additional parameters. Instead of modifying all parameters of a pretrained model, LoRA keeps the original weights frozen and introduces two low-rank matrices that approximate the necessary changes during training.\n",
    "\n",
    "```py\n",
    "W_new = W_original + ΔW\n",
    "ΔW = A × B\n",
    "```\n",
    "\n",
    "- W_original: Frozen pre-trained weights\n",
    "- A: Matrix of size (d × r)\n",
    "- B: Matrix of size (r × k)\n",
    "- r: Rank (much smaller than d and k)\n",
    "\n",
    "```py\n",
    "Original: h = W × x\n",
    "LoRA: h = W × x + (B × A) × x\n",
    "```\n",
    "\n",
    "This low-rank decomposition allows the model to adapt to new tasks with significantly fewer trainable parameters, reducing both memory usage and computational overhead during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2e46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Load a small model (adjust the model name as needed)\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15345e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer chat template if needed\n",
    "if not tokenizer.chat_template:\n",
    "    tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "            {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n",
    "            {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n",
    "            {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n",
    "            {% endif %}\n",
    "            {% endfor %}\"\"\"\n",
    "\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume train_dataset is already loaded and pre-processed,\n",
    "# for example, by using load_dataset from the datasets library.\n",
    "# Here we select a small subset for quick demonstration.\n",
    "USE_GPU = False\n",
    "if not USE_GPU:\n",
    "    train_dataset = train_dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb97754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SFT configuration along with LoRA hyperparameters:\n",
    "# Here, lora_r (rank) and lora_alpha (scaling factor) are added.\n",
    "sft_config = SFTConfig(\n",
    "    learning_rate=8e-5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=False,\n",
    "    logging_steps=2,\n",
    "    lora_r=8,        # Low-rank dimension\n",
    "    lora_alpha=16,   # Scaling factor for LoRA updates\n",
    ")\n",
    "\n",
    "# Create SFTTrainer with LoRA-enabled configuration\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# Start training\n",
    "sft_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-blogs-articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
