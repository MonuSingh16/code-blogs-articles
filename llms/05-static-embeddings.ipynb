{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8addfa",
   "metadata": {},
   "source": [
    "#### 01. Static Embeddings - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f98d1438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vector for 'bank':\n",
      " [-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809\n",
      "  0.06458873  0.08972988 -0.05015428 -0.03763372]\n",
      "Word2Vec : Bank's Word Vector Shape (10,)\n",
      "Similarity between 'bank' and 'river': 0.43182474\n",
      "Similarity between 'bank' and 'money': -0.1311161\n"
     ]
    }
   ],
   "source": [
    "# Example: Using Word2Vec with a small sample corpus\n",
    "\n",
    "# Import Word2Vec from gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Basic tokenizer: splits sentences into lowercase words\n",
    "def simple_tokenizer(text):\n",
    "    # Lowercase and split on spaces (very basic, for demo only)\n",
    "    return text.lower().replace('.', '').split()\n",
    "\n",
    "# Example sentences with the word 'bank' in different contexts\n",
    "raw_sentences = [\n",
    "    \"He sat by the river bank.\",\n",
    "    \"She deposited money in the bank.\",\n",
    "    \"The bank was closed on Sunday.\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [simple_tokenizer(sent) for sent in raw_sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "# vector_size: dimension of the embedding vectors (higher = more expressive, but needs more data)\n",
    "# min_count: ignore words with total frequency lower than this (set to 1 to include all words in this tiny corpus)\n",
    "# window: maximum distance between the current and predicted word within a sentence (context window size)\n",
    "w2v_model = Word2Vec(tokenized_sentences, vector_size=10, min_count=1, window=3)\n",
    "\n",
    "# Get embedding for 'bank'\n",
    "# The vector for 'bank' will be the same regardless of which sentence/context it appears in\n",
    "print(\"Word2Vec vector for 'bank':\\n\", w2v_model.wv['bank'])\n",
    "print(\"Word2Vec : Bank's Word Vector Shape\", w2v_model.wv['bank'].shape)\n",
    "\n",
    "words = list(w2v_model.wv.index_to_key)  # List all words in the vocabulary\n",
    "similar = w2v_model.wv.most_similar('bank')  # Find words most similar to 'bank'\n",
    "\n",
    "print(\"Similarity between 'bank' and 'river':\", w2v_model.wv.similarity('bank', 'river'))\n",
    "print(\"Similarity between 'bank' and 'money':\", w2v_model.wv.similarity('bank', 'money'))\n",
    "\n",
    "# NOTE:\n",
    "\n",
    "# - The model uses a sliding window (set by the window parameter) to look at neighboring words \n",
    "# - and learns that words appearing in similar contexts should have similar vectors.\n",
    "\n",
    "# - bank' has the same embedding in both \"river bank\" and \"money bank\" contexts.\n",
    "# - the word \"bank\" appears in both \"river bank\" and \"money bank\" contexts. \n",
    "# - The model tries to capture both, but since static embeddings can only assign one vector \n",
    "# - per word, \"bank\" gets a single vector that is an average of all its contexts.\n",
    "\n",
    "# - This is a limitation of static embeddings:\n",
    "# - they cannot distinguish between different meanings (senses) of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b57d7",
   "metadata": {},
   "source": [
    "#### 02. Static Embeddings - FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23872ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText vector for 'bank':\n",
      " [-0.02681367 -0.00070708  0.01805622  0.00670038 -0.00706677 -0.01905808\n",
      "  0.01269603  0.00208769  0.00945567 -0.02328101]\n",
      "FastText vector for OOV word 'banking':\n",
      " [ 0.00135347 -0.00299125  0.00331595  0.01845975  0.0230606   0.00761079\n",
      "  0.00130226  0.00841182  0.00700071 -0.01047717]\n",
      "FastText vector for OOV word 'bankzzz':\n",
      " [-0.02014219  0.00406886 -0.00836108  0.01079835  0.01020753  0.00953312\n",
      "  0.01484188 -0.01233316  0.0123117  -0.02921248]\n",
      "Similarity between 'bank' and 'banking': 0.112472326\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Train FastText model on your tokenized sentences\n",
    "ft_model = FastText(tokenized_sentences, vector_size=10, min_count=1, window=3)\n",
    "\n",
    "# Get embedding for a known word\n",
    "# Each word is broken into character n-grams (e.g., \"bank\" â†’ <ba, ban, ank, nk>, etc.).\n",
    "# The word vector is the sum (or average) of its n-gram vectors.\n",
    "# Training is similar to Word2Vec (CBOW or Skip-gram), but on subwords.\n",
    "print(\"FastText vector for 'bank':\\n\", ft_model.wv['bank'])\n",
    "\n",
    "# FastText can handle OOV (out-of-vocabulary) words using subword information\n",
    "print(\"FastText vector for OOV word 'banking':\\n\", ft_model.wv['banking'])\n",
    "\n",
    "# Compare with a nonsense word (still gets a vector!)\n",
    "print(\"FastText vector for OOV word 'bankzzz':\\n\", ft_model.wv['bankzzz'])\n",
    "\n",
    "# Demo: Similarity between 'bank' and 'banking'\n",
    "print(\"Similarity between 'bank' and 'banking':\", ft_model.wv.similarity('bank', 'banking'))\n",
    "\n",
    "# NOTE:\n",
    "# - FastText is especially useful when you expect to encounter new words,\n",
    "# - rare words, or work with morphologically rich languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79612057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-blogs-articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
