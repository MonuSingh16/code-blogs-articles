{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f60dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module): #inherits from nn.Module\n",
    "    def __init__(self, d_in, d_out): # contructor of the class\n",
    "        super().__init__() # intialize the parent class\n",
    "        # keyword self in a classs refers to the instance of the class\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        # create a layer that applies an affine transformation to the input\n",
    "        # y = Ax + b, where A is a weight matrix and b is a bias vector\n",
    "        # Weights intialized with a uniform distribution\n",
    "        # its weights and biases are stored as torch.nn.Parameter objects.\n",
    "        # This makes them part of the modelâ€™s .parameters() \n",
    "        # returns the parameters of the model when called\n",
    "        self.Q = nn.Linear(d_in, d_out) \n",
    "        self.K = nn.Linear(d_in, d_out)\n",
    "        self.V = nn.Linear(d_in, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.Q(x) # apply the affine transformation to the input x\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        # Compute the attention scores, bmm is batch matrix multiplication\n",
    "        # scores = queries * keys^T / sqrt(d_out)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) \n",
    "        # keys.transpose(1, 2) transposes the last two dimensions\n",
    "        # (batch_size, seq_len, d_out) -> (batch_size, d_out, seq_len)\n",
    "        scores = scores / (self.d_out ** 0.5)\n",
    "        attention = F.softmax(scores, dim=2)\n",
    "        # converts the attention scores into probabilities along the last dimension, \n",
    "        # so each set of scores sums to 1 for every query in the batch.\n",
    "        hidden_states = torch.bmm(attention, values)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2754e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65afcf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.out = nn.Linear(hidden_size, hidden_size)\n",
    "        self.head = nn.ModuleList([\n",
    "            Attention(hidden_size, hidden_size // num_heads)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "    \n",
    "    def foward(self, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
