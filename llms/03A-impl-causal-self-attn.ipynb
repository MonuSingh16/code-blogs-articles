{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0074bc",
   "metadata": {},
   "source": [
    "#### 1 Causal Attention -\n",
    "\n",
    "Only considers previous and current inputs in a sequnce when processing any given token when computing attention score, Self attention allows all tokens in an input sequence to be processed at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a5b293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5337, -0.1051],\n",
      "        [-0.5323, -0.1080],\n",
      "        [-0.5323, -0.1079],\n",
      "        [-0.5297, -0.1076],\n",
      "        [-0.5311, -0.1066],\n",
      "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Using nn.Linear has optimized weight initialization scheme,\n",
    "# leading better model training  (effective & Stable)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1)\n",
    "[0.55, 0.87, 0.66], # journey (x^2)\n",
    "[0.57, 0.85, 0.64], # starts (x^3)\n",
    "[0.22, 0.58, 0.33], # with (x^4)\n",
    "[0.77, 0.25, 0.10], # one (x^5)\n",
    "[0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1] ** 0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(3, 2)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d4ffb",
   "metadata": {},
   "source": [
    "##### 1.1 Applying a Causal Attention Mask\n",
    "\n",
    "Intially it might appear that mask attention weights and renormalize might still have influence on the current token beacuse thier values are still part of softmax calculations, But since we are re-normalizing masked attention weights, we are essentialy doing is recalculating softmax over the smaller subset (Elegance of Softmax,) Therefor No Information Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5735880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
      "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
      "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1. Reuses the query and key weight matrices of the SelfAttention_v2 object\n",
    "queries = sa_v2.W_q(inputs)\n",
    "keys = sa_v2.W_k(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(\n",
    "    attention_scores / keys.shape[-1] ** 0.5, dim=-1\n",
    ")\n",
    "print(attention_weights)\n",
    "#context_vectors = attention_weights @ sa_v2.W_v(inputs)\n",
    "\n",
    "# 2. Pytorch's tril function to create a simple mask\n",
    "context_length =  attention_scores.shape[0] \n",
    "simple_mask = torch.tril(torch.ones(context_length, context_length))\n",
    "#print(simple_mask)\n",
    "\n",
    "masked_simple = attention_weights * simple_mask # Multiply\n",
    "#print(masked_simple)\n",
    "\n",
    "# 3. Renormalize attention weights to sum up to 1 again in each row\n",
    "rows_sum = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / rows_sum\n",
    "#print(masked_simple_norm)\n",
    "\n",
    "# 4. A more efficient trick\n",
    "# Softmax converts its input into a probability dist. large negative -> 0\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "attention_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede7777",
   "metadata": {},
   "source": [
    "#### 1.2 Masking additonal attention weights with Dropout\n",
    "\n",
    "Randomly selected hidden layer unit are ignored during trainig. Prevents overfitting such that it does not become over reliant on any specific hidden layer units.\n",
    "\n",
    "Dropout in GPT is applied at two times - After calculating attention wieghts or after multiplying attention weights with value vectors\n",
    "\n",
    "When we apply dropuout to attention weights, Half of the elements in attention wieght matrix is set to 0. To compensate for the reduction in elements, the values of remaining element is scaled  by factor of 1 / 0.5 = 2, to maintain overall balance of attention wieghts, ensurng average influence of attention mechanism remain consistent during both training and inference stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bbafa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6816, 0.6804, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5085, 0.4936, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3906, 0.0000],\n",
      "        [0.3249, 0.3418, 0.0000, 0.3308, 0.3249, 0.3363]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))\n",
    "\n",
    "print(dropout(attention_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041f39a",
   "metadata": {},
   "source": [
    "#### 1.3 Implementing a compact class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609629d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# 2 inputs with 6 token each. each tokem has dimension = 3\n",
    "batch = torch.stack((inputs, inputs), dim=0) \n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e835a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                        diagonal=1)\n",
    "        )\n",
    "        # buffers are automatically moved to appropiate device, along with our model\n",
    "        # Hence no need to manually ensure that tensor on same device as model params\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # Keeping the batch at dimension  0\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1, 2) # Transposing dim 1 and 2\n",
    "        attention_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )# in PyTorch, Operation with trailing _ is performed in-place. \n",
    "        # Avoiding unnecessary memory copies\n",
    "\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1] ** 0.5, dim=-1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vec = attention_weights @ values\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(3, 2, context_length, 0.0)\n",
    "context_vec = ca(batch)\n",
    "print(\"context_vecs.shape\", context_vec.shape)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-blogs-articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
