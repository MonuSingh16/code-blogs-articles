{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# torch.manual_seed(1337) ensures our training results can be repeated. \n",
    "# That’s important when debugging or running comparisons, randomness in\n",
    "# weight initialization and batching can otherwise make runs inconsistent.\n",
    "torch.manual_seed(3)\n",
    "device = 'mps' if torch.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64 # Dimension of the hidden state\n",
    "transformer_layers = 6 # Number of transformer layers\n",
    "num_heads = 2 # Multi-head attention heads, lets model learn different representations of the input data\n",
    "block_size = 32 # No of inputs blocks model can see at once\n",
    "rms_norm_eps = 1e-5 # Epsilon value for RMS normalization\n",
    "rope_theta = 10000.0\n",
    "\n",
    "num_experts = 8 # Number of experts in the mixture of experts model per layer\n",
    "select_experts_per_token = 2 # Top-K gating, for every token, router picks two experts\n",
    "expert_output_dim = hidden_dim * 2 # Each expert MLP expands the token’s dimensionality to a wider hidden space before projecting it back down. \n",
    "shared_expert_output_dim = hidden_dim * 2 #  If we ever use shared FFNs, this would be their size. \n",
    "\n",
    "learning_rate = 5e-4\n",
    "batch_size = 32 # We process 32 sequences at a time during training.\n",
    "epochs = 1000\n",
    "eval_interval = 200\n",
    "\n",
    "assert hidden_dim % num_heads == 0, \"hidden dim is not divisible by num heads\"\n",
    "dim_k = hidden_dim // num_heads\n",
    "# Each attention head works on a chunk of the embedding, and this derived value defines how big that chunk is. For us: 128 / 4 = 32 per head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 347 characters.\n"
     ]
    }
   ],
   "source": [
    "input_data = (\"Facebook was founded in a dorm room \"\n",
    "              \"at Harvard by Mark Zuckerberg and \" \n",
    "              \"his roommates. What began as a small \"\n",
    "              \"social experiment soon transformed \"\n",
    "              \"into a global platform, connecting \"\n",
    "              \"billions of people across the world. \"\n",
    "              \"Over the years, it expanded beyond \"\n",
    "              \"profiles and friend requests, \"\n",
    "              \"introducing features like the News \"\n",
    "              \"Feed, Messenger, and Marketplace.\")\n",
    "# This is our training data, which is the raw material our model will learn patterns from.\n",
    "total_chars = len(input_data)\n",
    "\n",
    "print(f\"Training data has {total_chars} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0,\n",
      " ',': 1,\n",
      " '.': 2,\n",
      " 'F': 3,\n",
      " 'H': 4,\n",
      " 'M': 5,\n",
      " 'N': 6,\n",
      " 'O': 7,\n",
      " 'W': 8,\n",
      " 'Z': 9,\n",
      " 'a': 10,\n",
      " 'b': 11,\n",
      " 'c': 12,\n",
      " 'd': 13,\n",
      " 'e': 14,\n",
      " 'f': 15,\n",
      " 'g': 16,\n",
      " 'h': 17,\n",
      " 'i': 18,\n",
      " 'k': 19,\n",
      " 'l': 20,\n",
      " 'm': 21,\n",
      " 'n': 22,\n",
      " 'o': 23,\n",
      " 'p': 24,\n",
      " 'q': 25,\n",
      " 'r': 26,\n",
      " 's': 27,\n",
      " 't': 28,\n",
      " 'u': 29,\n",
      " 'v': 30,\n",
      " 'w': 31,\n",
      " 'x': 32,\n",
      " 'y': 33}\n",
      "{0: ' ',\n",
      " 1: ',',\n",
      " 2: '.',\n",
      " 3: 'F',\n",
      " 4: 'H',\n",
      " 5: 'M',\n",
      " 6: 'N',\n",
      " 7: 'O',\n",
      " 8: 'W',\n",
      " 9: 'Z',\n",
      " 10: 'a',\n",
      " 11: 'b',\n",
      " 12: 'c',\n",
      " 13: 'd',\n",
      " 14: 'e',\n",
      " 15: 'f',\n",
      " 16: 'g',\n",
      " 17: 'h',\n",
      " 18: 'i',\n",
      " 19: 'k',\n",
      " 20: 'l',\n",
      " 21: 'm',\n",
      " 22: 'n',\n",
      " 23: 'o',\n",
      " 24: 'p',\n",
      " 25: 'q',\n",
      " 26: 'r',\n",
      " 27: 's',\n",
      " 28: 't',\n",
      " 29: 'u',\n",
      " 30: 'v',\n",
      " 31: 'w',\n",
      " 32: 'x',\n",
      " 33: 'y'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(input_data))) # all unique characters\n",
    "# set() removes duplicates, sorted() sorts the characters\n",
    "# This gives us a list of unique characters in the input data.\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# We create a mapping from characters to integers and vice versa.\n",
    "char_to_int = {i:idx for idx,i in enumerate(chars)}\n",
    "\n",
    "# This is a dictionary that maps each character to a unique integer.\n",
    "int_to_char = {idx:i for idx,i in enumerate(chars)}\n",
    "\n",
    "pprint(char_to_int), pprint(int_to_char)\n",
    "\n",
    "# Corresponding charaecters to integers mapping in the input data\n",
    "# This is a list comprehension that iterates over each character in the input data\n",
    "encoded_input_data = [char_to_int[ch] for ch in input_data]\n",
    "\n",
    "# Convert the encoded input data to a PyTorch tensor\n",
    "input_data_tensor = torch.tensor(encoded_input_data, dtype=torch.long, device=device)\n",
    "\n",
    "len(input_data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3, 10, 12, 14, 11, 23, 23, 19,  0, 31, 10, 27,  0, 15, 23, 29, 22, 13,\n",
      "        14, 13,  0, 18, 22,  0, 10,  0, 13, 23, 26, 21,  0, 26],\n",
      "       device='mps:0')\n",
      "tensor([10, 12, 14, 11, 23, 23, 19,  0, 31, 10, 27,  0, 15, 23, 29, 22, 13, 14,\n",
      "        13,  0, 18, 22,  0, 10,  0, 13, 23, 26, 21,  0, 26, 23],\n",
      "       device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare training data (input-target pairs)\n",
    "# Given a sequence of tokens, the model tries to predict the very next one.\n",
    "# In every training example, we give it a chunk of tokens (x) and ask it to predict the next chunk (y),\n",
    "# which is just x shifted one position forward.\n",
    "input_x = []\n",
    "output_y = []\n",
    "\n",
    "for i in range(len(input_data_tensor) - block_size):\n",
    "    \n",
    "    x = input_data_tensor[i : i + block_size]\n",
    "    y = input_data_tensor[i + 1 : i + block_size + 1]\n",
    "    \n",
    "    input_x.append(x)\n",
    "    output_y.append(y)\n",
    "\n",
    "# torch.stack these lists to create two large tensors, one for all inputs and one for all targets.\n",
    "train_x = torch.stack(input_x)\n",
    "train_y = torch.stack(output_y)\n",
    "\n",
    "print(input_x[0]), print(output_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(34, 64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maps each vocab_size unique tokens to a hidden_dim sized vector\n",
    "# When we pass in a batch of token sequences (shape: B x T), \n",
    "# the output is a batch of embeddings (shape: B x T x d_model). \n",
    "character_embedding_map = nn.Embedding(vocab_size, hidden_dim).to(device)\n",
    "character_embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates 1D positional encodings for each token in the input sequence.\n",
    "# Generates numbers starting from 0 up to dim_k, with a step of 2.\n",
    "\n",
    "# RoPE encodes position by rotating pairs of features inside query (Q) \n",
    "# and key (K) vectors. Instead of tacking on a separate position vector, \n",
    "# it twists the data inside Q and K using position-dependent angles.\n",
    "rope_range = torch.arange(0, dim_k, 2, dtype=torch.float, device=device)\n",
    "rope_freqs = 1.0 / (rope_theta ** (rope_range / dim_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS layer - Normalizes based on the root mean square of the input.\n",
    "# Gamma has learnaable parameter that allow the model to stretch or compress the normalized values.\n",
    "# x / sqrt(E[x^2] + eps) * gamma + beta\n",
    "\n",
    "# Two learnable paramter for each transformer layer, intialized to 1, and shape of (hidden_dim,)\n",
    "# Act as gamma scaling factors in RMS normalization for each layer.\n",
    "rms_before_attn = []\n",
    "rms_after_attn = []\n",
    "\n",
    "for i in range(transformer_layers):\n",
    "\n",
    "    weight_before_attn = nn.Parameter(torch.ones(hidden_dim, device=device))\n",
    "    rms_before_attn.append(weight_before_attn)\n",
    "\n",
    "    weight_after_attn = nn.Parameter(torch.ones(hidden_dim, device=device))\n",
    "    rms_after_attn.append(weight_after_attn)\n",
    "    \n",
    "# The final output of the transformer layer is also normalized.\n",
    "rms_before_final_output = nn.Parameter(torch.ones(hidden_dim, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention meaning each token will have multiple Q/K/V vectors.\n",
    "# Each head will have its own set of Q/K/V matrices, and the output of all heads will be concatenated.\n",
    "\n",
    "multi_head_layers = []\n",
    "feed_forward_layers = []\n",
    "\n",
    "# for each transformer layer.\n",
    "for i in range(transformer_layers):\n",
    "\n",
    "    # single layer to project from d_model to 3 * d_model\n",
    "    qkv_layer = nn.Linear(hidden_dim, 3 * hidden_dim, bias=False).to(device)\n",
    "    multi_head_layers.append(qkv_layer)\n",
    "\n",
    "    feed_forw_layer = nn.Linear(hidden_dim, hidden_dim, bias=False).to(device)\n",
    "    feed_forward_layers.append(feed_forw_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_layer = [] \n",
    "expert_feedforward_first_layer = []\n",
    "expert_feedforward_second_layer = []  \n",
    "\n",
    "shared_expert_layer = []\n",
    "shared_expert_feedforward_first_layer = []\n",
    "shared_expert_feedforward_second_layer = [] \n",
    "\n",
    "\n",
    "for i in range(transformer_layers):\n",
    "\n",
    "    # Router layer that decides which experts to use for each token.\n",
    "    # It takes the hidden state of the token and outputs a score for each expert.\n",
    "    router_linear_layer = nn.Linear(hidden_dim, num_experts, bias=False).to(device)\n",
    "    router_layer.append(router_linear_layer)\n",
    "\n",
    "    # Each expert has its own feedforward network.\n",
    "    # The first layer expands the token’s dimensionality to a wider hidden space.\n",
    "    # The second layer projects it back down to the original hidden_dim.\n",
    "    # Wraos the tensors as a learnable parameter, so they can be updated during training.\n",
    "    expert_first_layer = nn.Parameter(torch.empty(num_experts, hidden_dim, 2 * expert_output_dim, device=device))\n",
    "    nn.init.normal_(expert_first_layer, mean=0.0, std=0.05)\n",
    "    expert_feedforward_first_layer.append(expert_first_layer)\n",
    "\n",
    "    expert_second_layer = nn.Parameter(torch.empty(num_experts, expert_output_dim, hidden_dim, device=device))\n",
    "    nn.init.normal_(expert_second_layer, mean=0.0, std=0.05)\n",
    "    expert_feedforward_second_layer.append(expert_second_layer)\n",
    "\n",
    "    # Shared feedforward layers for the experts.\n",
    "    shared_gate = nn.Linear(hidden_dim, shared_expert_output_dim, bias=False).to(device)\n",
    "    shared_feedforward_first_layer = nn.Linear(hidden_dim, shared_expert_output_dim, bias=False).to(device)\n",
    "    shared_feedforward_second_layer = nn.Linear(shared_expert_output_dim, hidden_dim, bias=False).to(device)\n",
    "    \n",
    "    shared_expert_layer.append(shared_gate)\n",
    "    shared_expert_feedforward_first_layer.append(shared_feedforward_first_layer)\n",
    "    shared_expert_feedforward_second_layer.append(shared_feedforward_second_layer)\n",
    "\n",
    "activation_fn = nn.SiLU()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is called a language modeling head, which is a linear layer that converts \n",
    "# the final hidden state of each token into a vector of size vocab_size.\n",
    "language_modeling_head = nn.Linear(hidden_dim, vocab_size, bias=False).to(device)\n",
    "\n",
    "# These scores are called logits. \n",
    "# They’re unnormalized, meaning they don’t sum to 1,\n",
    "# that happens later via softmax during training or sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]], device='mps:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When generating text autoregressively, \n",
    "# one rule is meant to be followed, which is that the current token \n",
    "# shouldn't be able to look at future tokens.\n",
    "# This is done by creating a lower triangular matrix of ones,\n",
    "# which is then reshaped to match the dimensions of the attention scores.\n",
    "\n",
    "# max sequence length (block_size)\n",
    "# What is block_size?\n",
    "# block_size is the maximum sequence length that the model can process at once.\n",
    "# In language models, this is the maximum number of tokens (words, subwords, or characters) in an input sequence.\n",
    "# For example, if block_size = 128, the model can handle up to 128 tokens per input.\n",
    "lm_mask = torch.tril(torch.ones(block_size, block_size, device=device))\n",
    "lm_mask = lm_mask.view(1, 1, block_size, block_size)\n",
    "# Reshapes the mask to add batch and head dimensions,\n",
    "# making it compatible with multi-head attention layers.\n",
    "\n",
    "\n",
    "'''\n",
    "Why is the causal mask shaped (1, 1, block_size, block_size)?\n",
    "\n",
    "During attention computation, you deal with tensors shaped like:\n",
    "\n",
    "(B, n_heads, T, T) → one attention matrix per head and per sequence.\n",
    "By shaping the mask as (1, 1, block_size, block_size),\n",
    "\n",
    "we make it broadcastable across:\n",
    "\n",
    "All batches (B)\n",
    "All attention heads (n_heads)\n",
    "All sequence lengths (T)\n",
    "'''\n",
    "lm_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 2,176\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "trainable_parameters = list(character_embedding_map.parameters())\n",
    "\n",
    "total_params = sum(p.numel() for p in trainable_parameters if p.requires_grad)\n",
    "\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 3,008\n"
     ]
    }
   ],
   "source": [
    "trainable_parameters.extend(rms_before_attn)\n",
    "trainable_parameters.extend(rms_after_attn)\n",
    "trainable_parameters.append(rms_before_final_output)\n",
    "\n",
    "total_params = sum(p.numel() for p in trainable_parameters if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 101,312\n"
     ]
    }
   ],
   "source": [
    "for i in range(transformer_layers):\n",
    "    trainable_parameters.extend(list(multi_head_layers[i].parameters()))\n",
    "    trainable_parameters.extend(list(feed_forward_layers[i].parameters()))\n",
    "\n",
    "total_params = sum(p.numel() for p in trainable_parameters if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 1,431,488\n"
     ]
    }
   ],
   "source": [
    "for i in range(transformer_layers):\n",
    "    trainable_parameters.extend(list(router_layer[i].parameters()))\n",
    "\n",
    "trainable_parameters.extend(expert_feedforward_first_layer)\n",
    "trainable_parameters.extend(expert_feedforward_second_layer)\n",
    "\n",
    "for i in range(transformer_layers):\n",
    "    trainable_parameters.extend(list(shared_expert_layer[i].parameters()))\n",
    "    trainable_parameters.extend(list(shared_expert_feedforward_first_layer[i].parameters()))\n",
    "    trainable_parameters.extend(list(shared_expert_feedforward_second_layer[i].parameters()))\n",
    "\n",
    "total_params = sum(p.numel() for p in trainable_parameters if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 1,433,664\n"
     ]
    }
   ],
   "source": [
    "trainable_parameters.extend(list(language_modeling_head.parameters()))\n",
    "\n",
    "total_params = sum(p.numel() for p in trainable_parameters if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW, a variant of Adam with decoupled weight decay. \n",
    "# It’s the go-to optimizer for most Transformer models. \n",
    "# It is stable, fast-converging, and well-suited for handling sparse gradients\n",
    "# and scale-sensitive layers like LayerNorm and embeddings.\n",
    "optimizer = optim.AdamW(trainable_parameters, lr=learning_rate)\n",
    "\n",
    "# Normally, when you define a model using nn.Module, \n",
    "# you just call model.parameters() and pass that to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we’re training a language model that predicts the next token \n",
    "# at each position, the problem boils down to multi-class classification\n",
    "# at every timestep.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Lastly, you don’t need to call softmax() separately.\n",
    "#  The nn.CrossEntropyLoss() takes care of it internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token in the input sequence, the model outputs a logits vector, which is a raw score for every token in the vocabulary (e.g., 100+ possibilities for character-level, or 50,000+ for BPE models).\n",
    "\n",
    "The loss function:\n",
    "\n",
    "- Takes the predicted logits,\n",
    "- Compares them against the true next-token ID (the ground truth),\n",
    "- And computes how far off the model's guess was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/1000, Loss: 3.7216\n",
      "  Epoch 3/1000, Loss: 3.4610\n",
      "  Epoch 5/1000, Loss: 3.2192\n",
      "  Epoch 7/1000, Loss: 3.0446\n",
      "  Epoch 9/1000, Loss: 2.9065\n",
      "  Epoch 11/1000, Loss: 2.7757\n",
      "  Epoch 13/1000, Loss: 2.7103\n",
      "  Epoch 15/1000, Loss: 2.6119\n",
      "  Epoch 17/1000, Loss: 2.5643\n",
      "  Epoch 19/1000, Loss: 2.4804\n",
      "  Epoch 21/1000, Loss: 2.3923\n",
      "  Epoch 23/1000, Loss: 2.3690\n",
      "  Epoch 25/1000, Loss: 2.3463\n",
      "  Epoch 27/1000, Loss: 2.3284\n",
      "  Epoch 29/1000, Loss: 2.2412\n",
      "  Epoch 31/1000, Loss: 2.1638\n",
      "  Epoch 33/1000, Loss: 2.1516\n",
      "  Epoch 35/1000, Loss: 2.1241\n",
      "  Epoch 37/1000, Loss: 2.0961\n",
      "  Epoch 39/1000, Loss: 2.1250\n",
      "  Epoch 41/1000, Loss: 2.0754\n",
      "  Epoch 43/1000, Loss: 2.0292\n",
      "  Epoch 45/1000, Loss: 1.9590\n",
      "  Epoch 47/1000, Loss: 1.9311\n",
      "  Epoch 49/1000, Loss: 1.8731\n",
      "  Epoch 51/1000, Loss: 1.7785\n",
      "  Epoch 53/1000, Loss: 1.7195\n",
      "  Epoch 55/1000, Loss: 1.6347\n",
      "  Epoch 57/1000, Loss: 1.5867\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 139\u001b[39m\n\u001b[32m    137\u001b[39m optimizer.zero_grad()\n\u001b[32m    138\u001b[39m loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m current_loss_val = loss.item()\n\u001b[32m    142\u001b[39m epoch_losses.append(current_loss_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work-share/code-blogs-articles/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work-share/code-blogs-articles/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work-share/code-blogs-articles/.venv/lib/python3.12/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work-share/code-blogs-articles/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work-share/code-blogs-articles/.venv/lib/python3.12/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work-share/code-blogs-articles/.venv/lib/python3.12/site-packages/torch/optim/adam.py:456\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    454\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m.addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    459\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "total_samples = len(train_x)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    \n",
    "    # Each element is a random integer between 0 (inclusive) and total_samples (exclusive).\n",
    "    # torch.randint(0, total_samples, (batch_size,)) generates a 1D tensor of length batch_size.\n",
    "    batch_indices = torch.randint(0, total_samples, (batch_size,))\n",
    "    \n",
    "    input_tokens = train_x[batch_indices].to(device)\n",
    "    target_tokens = train_y[batch_indices].to(device)\n",
    "\n",
    "    bsz = input_tokens.shape[0]\n",
    "    seq_len = input_tokens.shape[1]\n",
    "    model_dim = hidden_dim\n",
    "\n",
    "    embedded_tokens = character_embedding_map(input_tokens)\n",
    "\n",
    "    # .unsqueeze(0) adds a new dimension at the front, changing the shape from (seq_len,) to (1, seq_len).\n",
    "    # The extra dimension allows easy broadcasting when combining with batch\n",
    "    pos_indices = torch.arange(seq_len, device=device).unsqueeze(0)\n",
    "\n",
    "    # .unsqueeze(0) adds a batch dimension at the front: shape becomes (1, N).\n",
    "    # .unsqueeze(-1) adds a singleton dimension at the end: shape becomes (1, N, 1).\n",
    "    # .expand(bsz, -1, 1) repeats the tensor along the batch dimension (bsz times),\n",
    "    #  keeping the other dimensions unchanged (-1 means \"keep original size\").\n",
    "    inv_freq_broadcasted = rope_freqs.unsqueeze(0).unsqueeze(-1).expand(bsz, -1, 1)\n",
    "    pos_ids_broadcasted = pos_indices.expand(bsz, -1).unsqueeze(1).float()\n",
    "\n",
    "    with torch.autocast(device_type=device, enabled=False):\n",
    "        angle_matrix = (inv_freq_broadcasted.float() @ pos_ids_broadcasted).transpose(1, 2)\n",
    "        freqs_complex = torch.polar(torch.ones_like(angle_matrix), angle_matrix)\n",
    "\n",
    "    layer_input = embedded_tokens\n",
    "\n",
    "    for layer_id in range(transformer_layers):\n",
    "\n",
    "        attn_residual = layer_input\n",
    "\n",
    "        x_fp32 = layer_input.float()\n",
    "        rms_norm_factor = torch.rsqrt(x_fp32.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "        normed_attn_input = (x_fp32 * rms_norm_factor).type_as(layer_input)\n",
    "        normed_attn_input = normed_attn_input * rms_before_attn[layer_id]\n",
    "\n",
    "        qkv_combined = multi_head_layers[layer_id](normed_attn_input)\n",
    "        qkv_combined = qkv_combined.view(bsz, seq_len, num_heads, 3 * dim_k)\n",
    "        queries, keys, values = qkv_combined.chunk(3, dim=-1)\n",
    "\n",
    "        queries_reshaped = queries.float().reshape(bsz, seq_len, num_heads, -1, 2)\n",
    "        keys_reshaped = keys.float().reshape(bsz, seq_len, num_heads, -1, 2)\n",
    "\n",
    "        queries_complex = torch.view_as_complex(queries_reshaped)\n",
    "        keys_complex = torch.view_as_complex(keys_reshaped)\n",
    "        freq_broadcast = freqs_complex.unsqueeze(2)\n",
    "\n",
    "        queries_rot = queries_complex * freq_broadcast\n",
    "        keys_rot = keys_complex * freq_broadcast\n",
    "\n",
    "        queries_real = torch.view_as_real(queries_rot)\n",
    "        keys_real = torch.view_as_real(keys_rot)\n",
    "\n",
    "        queries = queries_real.flatten(3).type_as(queries)\n",
    "        keys = keys_real.flatten(3).type_as(keys)\n",
    "\n",
    "        queries = queries.permute(0, 2, 1, 3)\n",
    "        keys = keys.permute(0, 2, 1, 3)\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "\n",
    "        attention_scores = (queries @ keys.transpose(-2, -1)) * (dim_k ** -0.5)\n",
    "        attention_scores = attention_scores.masked_fill(lm_mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "\n",
    "        attn_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attn_probs = torch.nan_to_num(attn_probs)\n",
    "\n",
    "        attn_out = attn_probs @ values\n",
    "        attn_out = attn_out.permute(0, 2, 1, 3).contiguous().view(bsz, seq_len, model_dim)\n",
    "\n",
    "        attn_out = feed_forward_layers[layer_id](attn_out)\n",
    "\n",
    "        layer_input = attn_residual + attn_out\n",
    "        moe_residual = layer_input\n",
    "\n",
    "        x_fp32 = layer_input.float()\n",
    "        rms_norm_factor = torch.rsqrt(x_fp32.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "        normed_moe_input = (x_fp32 * rms_norm_factor).type_as(layer_input)\n",
    "        normed_moe_input = normed_moe_input * rms_after_attn[layer_id]\n",
    "\n",
    "        route_logits = router_layer[layer_id](normed_moe_input)\n",
    "\n",
    "        expert_scores, expert_indices = torch.topk(route_logits, select_experts_per_token, dim=-1)\n",
    "        expert_scores = torch.sigmoid(expert_scores)\n",
    "\n",
    "        flattened_input = normed_moe_input.view(-1, model_dim)\n",
    "        flattened_expert_ids = expert_indices.view(-1)\n",
    "        flattened_scores = expert_scores.view(-1)\n",
    "\n",
    "        token_positions = torch.arange(bsz * seq_len, device=device).repeat_interleave(select_experts_per_token)\n",
    "        expert_routing = flattened_expert_ids\n",
    "\n",
    "        expert_inputs = flattened_input[token_positions]\n",
    "\n",
    "        expert_up_weights = expert_feedforward_first_layer[layer_id][expert_routing]\n",
    "        expert_down_weights = expert_feedforward_second_layer[layer_id][expert_routing]\n",
    "\n",
    "        intermediate = torch.bmm(expert_inputs.unsqueeze(1), expert_up_weights)\n",
    "        gate_vals, up_vals = intermediate.chunk(2, dim=-1)\n",
    "\n",
    "        expert_activated = activation_fn(gate_vals) * up_vals\n",
    "        expert_raw_output = torch.bmm(expert_activated, expert_down_weights).squeeze(1)\n",
    "        expert_scaled_output = expert_raw_output * flattened_scores.unsqueeze(-1)\n",
    "\n",
    "        combined_outputs = torch.zeros_like(flattened_input)\n",
    "        combined_outputs.scatter_add_(0, token_positions.unsqueeze(-1).expand(-1, model_dim), expert_scaled_output)\n",
    "\n",
    "        shared_gate = shared_expert_layer[layer_id](normed_moe_input)\n",
    "        shared_up = shared_expert_feedforward_first_layer[layer_id](normed_moe_input)\n",
    "        shared_activated = activation_fn(shared_gate) * shared_up\n",
    "        shared_down = shared_expert_feedforward_second_layer[layer_id](shared_activated)\n",
    "\n",
    "        moe_out = combined_outputs.view(bsz, seq_len, model_dim)\n",
    "        moe_combined = moe_out + shared_down\n",
    "\n",
    "        layer_input = moe_residual + moe_combined\n",
    "\n",
    "    x_fp32 = layer_input.float()\n",
    "    rms_norm_factor = torch.rsqrt(x_fp32.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    final_norm = (x_fp32 * rms_norm_factor).type_as(layer_input)\n",
    "    final_norm = final_norm * rms_before_final_output\n",
    "\n",
    "    vocab_logits = language_modeling_head(final_norm)\n",
    "\n",
    "    B, T, V = vocab_logits.shape\n",
    "    logits_flat = vocab_logits.view(B * T, V)\n",
    "    targets_flat = target_tokens.view(B * T)\n",
    "    loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    current_loss_val = loss.item()\n",
    "    epoch_losses.append(current_loss_val)\n",
    "    if ep % 2 == 0 or ep == epochs - 1:\n",
    "        print(f\"  Epoch {ep+1}/{epochs}, Loss: {current_loss_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(epoch_losses)\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=64, out_features=34, bias=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"Facebook \" \n",
    "\n",
    "generation_limit = 100\n",
    "\n",
    "prompt_token_ids = [char_to_int[ch] for ch in prompt_text if ch in char_to_int]\n",
    "gen_sequence_buffer = torch.tensor([prompt_token_ids], dtype=torch.long, device=device)\n",
    "\n",
    "character_embedding_map.eval()\n",
    "for layer_idx in range(transformer_layers):\n",
    "    multi_head_layers[layer_idx].eval()\n",
    "    feed_forward_layers[layer_idx].eval()\n",
    "    router_layer[layer_idx].eval()\n",
    "    shared_expert_layer[layer_idx].eval()\n",
    "    shared_expert_feedforward_first_layer[layer_idx].eval()\n",
    "    shared_expert_feedforward_second_layer[layer_idx].eval()\n",
    "\n",
    "language_modeling_head.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for _ in range(generation_limit):\n",
    "        \n",
    "        input_context = gen_sequence_buffer[:, -block_size:] \n",
    "        batch_size_gen, seq_len_gen = input_context.shape\n",
    "        hidden_size_gen = hidden_dim\n",
    "\n",
    "        embedded_input = character_embedding_map(input_context) \n",
    "\n",
    "        position_indices = torch.arange(seq_len_gen, device=device).unsqueeze(0)\n",
    "        inv_freq_broadcast = rope_freqs.unsqueeze(0).unsqueeze(-1).expand(batch_size_gen, -1, 1)\n",
    "        pos_ids_broadcast = position_indices.expand(batch_size_gen, -1).unsqueeze(1).float()\n",
    "        \n",
    "        with torch.autocast(device_type=device, enabled=False):\n",
    "            pos_angles = (inv_freq_broadcast.float() @ pos_ids_broadcast).transpose(1, 2)\n",
    "            rope_rotations = torch.polar(torch.ones_like(pos_angles), pos_angles)\n",
    "\n",
    "        layer_input = embedded_input\n",
    "\n",
    "        for layer_idx in range(transformer_layers):\n",
    "            attn_skip = layer_input\n",
    "\n",
    "            x_fp32 = layer_input.float()\n",
    "            norm_factor = torch.rsqrt(x_fp32.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "            normed_input = (x_fp32 * norm_factor).type_as(layer_input)\n",
    "            normed_input = normed_input * rms_before_attn[layer_idx]\n",
    "\n",
    "            qkv_proj = multi_head_layers[layer_idx](normed_input)\n",
    "            qkv_proj = qkv_proj.view(batch_size_gen, seq_len_gen, num_heads, 3 * dim_k)\n",
    "            query, key, value = qkv_proj.chunk(3, dim=-1)\n",
    "\n",
    "            query_cmplx = torch.view_as_complex(query.float().reshape(batch_size_gen, seq_len_gen, num_heads, -1, 2))\n",
    "            key_cmplx = torch.view_as_complex(key.float().reshape(batch_size_gen, seq_len_gen, num_heads, -1, 2))\n",
    "            rope_cis_broadcast = rope_rotations.unsqueeze(2)\n",
    "            query_rot = query_cmplx * rope_cis_broadcast\n",
    "            key_rot = key_cmplx * rope_cis_broadcast\n",
    "            query = torch.view_as_real(query_rot).flatten(3).type_as(query)\n",
    "            key = torch.view_as_real(key_rot).flatten(3).type_as(key)\n",
    "\n",
    "            query = query.permute(0, 2, 1, 3)\n",
    "            key = key.permute(0, 2, 1, 3)\n",
    "            value = value.permute(0, 2, 1, 3)\n",
    "            attn_scores = (query @ key.transpose(-2, -1)) * (dim_k ** -0.5)\n",
    "            attn_scores = attn_scores.masked_fill(lm_mask[:, :, :seq_len_gen, :seq_len_gen] == 0, float('-inf'))\n",
    "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "            attn_probs = torch.nan_to_num(attn_probs)\n",
    "            attn_out = attn_probs @ value\n",
    "            attn_out = attn_out.permute(0, 2, 1, 3).contiguous().view(batch_size_gen, seq_len_gen, hidden_size_gen)\n",
    "            attn_out = feed_forward_layers[layer_idx](attn_out)\n",
    "\n",
    "            layer_input = attn_skip + attn_out\n",
    "\n",
    "            moe_skip = layer_input\n",
    "\n",
    "            x_fp32 = layer_input.float()\n",
    "            norm_factor = torch.rsqrt(x_fp32.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "            normed_input = (x_fp32 * norm_factor).type_as(layer_input)\n",
    "            normed_input = normed_input * rms_after_attn[layer_idx]\n",
    "\n",
    "            route_logits = router_layer[layer_idx](normed_input)\n",
    "            topk_weights, topk_indices = torch.topk(route_logits, select_experts_per_token, dim=-1)\n",
    "            topk_weights = torch.sigmoid(topk_weights)\n",
    "\n",
    "            flattened_input = normed_input.view(-1, hidden_size_gen)\n",
    "            flattened_expert_ids = topk_indices.view(-1)\n",
    "            flattened_weights = topk_weights.view(-1)\n",
    "            token_positions = torch.arange(batch_size_gen * seq_len_gen, device=device).repeat_interleave(select_experts_per_token)\n",
    "\n",
    "            expert_inputs = flattened_input[token_positions]\n",
    "            gate_up_weights = expert_feedforward_first_layer[layer_idx][flattened_expert_ids]\n",
    "            down_weights = expert_feedforward_second_layer[layer_idx][flattened_expert_ids]\n",
    "\n",
    "            up_states = torch.bmm(expert_inputs.unsqueeze(1), gate_up_weights)\n",
    "            gate_vals, up_vals = up_states.chunk(2, dim=-1)\n",
    "            activated = activation_fn(gate_vals) * up_vals\n",
    "            expert_out_flat = torch.bmm(activated, down_weights).squeeze(1)\n",
    "            scaled_expert_out = expert_out_flat * flattened_weights.unsqueeze(-1)\n",
    "\n",
    "            combined_expert_out = torch.zeros_like(flattened_input)\n",
    "            combined_expert_out.scatter_add_(\n",
    "                0, token_positions.unsqueeze(-1).expand(-1, hidden_size_gen), scaled_expert_out\n",
    "            )\n",
    "\n",
    "            shared_gate = shared_expert_layer[layer_idx](normed_input)\n",
    "            shared_up = shared_expert_feedforward_first_layer[layer_idx](normed_input)\n",
    "            shared_activated = activation_fn(shared_gate) * shared_up\n",
    "            shared_out = shared_expert_feedforward_second_layer[layer_idx](shared_activated)\n",
    "\n",
    "            moe_combined = combined_expert_out.view(batch_size_gen, seq_len_gen, hidden_size_gen)\n",
    "            final_moe_out = moe_combined + shared_out\n",
    "\n",
    "            layer_input = moe_skip + final_moe_out\n",
    "\n",
    "        x_fp32 = layer_input.float()\n",
    "        norm_factor = torch.rsqrt(x_fp32.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "        final_norm = (x_fp32 * norm_factor).type_as(layer_input)\n",
    "        final_norm = final_norm * rms_before_final_output\n",
    "\n",
    "        vocab_logits = language_modeling_head(final_norm)\n",
    "        logits_last_step = vocab_logits[:, -1, :] \n",
    "\n",
    "        token_probs = F.softmax(logits_last_step, dim=-1)\n",
    "        sampled_token = torch.multinomial(token_probs, num_samples=1)\n",
    "\n",
    "        gen_sequence_buffer = torch.cat((gen_sequence_buffer, sampled_token), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3, 10, 12, 14, 11, 23, 23, 19,  0, 31, 10, 27,  0, 15, 23, 29, 22, 13,\n",
       "         14, 13,  0, 18, 22,  0, 10,  0, 13, 23, 26, 21,  0, 26, 23, 23, 21,  0,\n",
       "         10, 28,  0,  4, 10, 26, 30, 10, 26, 13,  0, 11, 33,  0,  5, 10, 26, 19,\n",
       "          0,  9, 29, 12, 19, 14, 26, 11, 14, 26, 16,  0, 10, 22, 13,  0, 17, 18,\n",
       "         27,  0, 26, 23, 23, 21, 21, 10, 28, 14, 27,  2,  0,  8, 17, 10, 28,  0,\n",
       "         11, 14, 16, 10, 22,  0, 10, 27,  0, 10,  0, 27, 21, 10, 20, 20,  0, 27,\n",
       "         23]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_sequence_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook was founded in a dorm room at Harvard by Mark Zuckerberg and his roommates. What began as a small so\n"
     ]
    }
   ],
   "source": [
    "final_generated_ids = gen_sequence_buffer[0].tolist()\n",
    "\n",
    "decoded_text = ''.join([int_to_char.get(id_val, '[UNK]') for id_val in final_generated_ids])\n",
    "\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=64, out_features=34, bias=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"Dorm rooms are \" \n",
    "\n",
    "generation_limit = 100\n",
    "\n",
    "prompt_token_ids = [char_to_int[ch] for ch in prompt_text if ch in char_to_int]\n",
    "gen_sequence_buffer = torch.tensor([prompt_token_ids], dtype=torch.long, device=device)\n",
    "\n",
    "character_embedding_map.eval()\n",
    "for layer_idx in range(transformer_layers):\n",
    "    multi_head_layers[layer_idx].eval()\n",
    "    feed_forward_layers[layer_idx].eval()\n",
    "    router_layer[layer_idx].eval()\n",
    "    shared_expert_layer[layer_idx].eval()\n",
    "    shared_expert_feedforward_first_layer[layer_idx].eval()\n",
    "    shared_expert_feedforward_second_layer[layer_idx].eval()\n",
    "\n",
    "language_modeling_head.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Generation loop finished.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for _ in range(generation_limit):\n",
    "        \n",
    "        input_context = gen_sequence_buffer[:, -block_size:] \n",
    "        batch_size_gen, seq_len_gen = input_context.shape\n",
    "        hidden_size_gen = hidden_dim\n",
    "\n",
    "        embedded_input = character_embedding_map(input_context) \n",
    "\n",
    "        position_indices = torch.arange(seq_len_gen, device=device).unsqueeze(0)\n",
    "        inv_freq_broadcast = rope_freqs.unsqueeze(0).unsqueeze(-1).expand(batch_size_gen, -1, 1)\n",
    "        pos_ids_broadcast = position_indices.expand(batch_size_gen, -1).unsqueeze(1).float()\n",
    "        \n",
    "        with torch.autocast(device_type=device, enabled=False):\n",
    "            pos_angles = (inv_freq_broadcast.float() @ pos_ids_broadcast).transpose(1, 2)\n",
    "            rope_rotations = torch.polar(torch.ones_like(pos_angles), pos_angles)\n",
    "\n",
    "        layer_input = embedded_input\n",
    "\n",
    "        for layer_idx in range(transformer_layers):\n",
    "            attn_skip = layer_input\n",
    "\n",
    "            x_fp32 = layer_input.float()\n",
    "            norm_factor = torch.rsqrt(x_fp32.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "            normed_input = (x_fp32 * norm_factor).type_as(layer_input)\n",
    "            normed_input = normed_input * rms_before_attn[layer_idx]\n",
    "\n",
    "            qkv_proj = multi_head_layers[layer_idx](normed_input)\n",
    "            qkv_proj = qkv_proj.view(batch_size_gen, seq_len_gen, num_heads, 3 * dim_k)\n",
    "            query, key, value = qkv_proj.chunk(3, dim=-1)\n",
    "\n",
    "            query_cmplx = torch.view_as_complex(query.float().reshape(batch_size_gen, seq_len_gen, num_heads, -1, 2))\n",
    "            key_cmplx = torch.view_as_complex(key.float().reshape(batch_size_gen, seq_len_gen, num_heads, -1, 2))\n",
    "            rope_cis_broadcast = rope_rotations.unsqueeze(2)\n",
    "            query_rot = query_cmplx * rope_cis_broadcast\n",
    "            key_rot = key_cmplx * rope_cis_broadcast\n",
    "            query = torch.view_as_real(query_rot).flatten(3).type_as(query)\n",
    "            key = torch.view_as_real(key_rot).flatten(3).type_as(key)\n",
    "\n",
    "            query = query.permute(0, 2, 1, 3)\n",
    "            key = key.permute(0, 2, 1, 3)\n",
    "            value = value.permute(0, 2, 1, 3)\n",
    "            attn_scores = (query @ key.transpose(-2, -1)) * (dim_k ** -0.5)\n",
    "            attn_scores = attn_scores.masked_fill(lm_mask[:, :, :seq_len_gen, :seq_len_gen] == 0, float('-inf'))\n",
    "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "            attn_probs = torch.nan_to_num(attn_probs)\n",
    "            attn_out = attn_probs @ value\n",
    "            attn_out = attn_out.permute(0, 2, 1, 3).contiguous().view(batch_size_gen, seq_len_gen, hidden_size_gen)\n",
    "            attn_out = feed_forward_layers[layer_idx](attn_out)\n",
    "\n",
    "            layer_input = attn_skip + attn_out\n",
    "\n",
    "            moe_skip = layer_input\n",
    "\n",
    "            x_fp32 = layer_input.float()\n",
    "            norm_factor = torch.rsqrt(x_fp32.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "            normed_input = (x_fp32 * norm_factor).type_as(layer_input)\n",
    "            normed_input = normed_input * rms_after_attn[layer_idx]\n",
    "\n",
    "            route_logits = router_layer[layer_idx](normed_input)\n",
    "            topk_weights, topk_indices = torch.topk(route_logits, select_experts_per_token, dim=-1)\n",
    "            topk_weights = torch.sigmoid(topk_weights)\n",
    "\n",
    "            flattened_input = normed_input.view(-1, hidden_size_gen)\n",
    "            flattened_expert_ids = topk_indices.view(-1)\n",
    "            flattened_weights = topk_weights.view(-1)\n",
    "            token_positions = torch.arange(batch_size_gen * seq_len_gen, device=device).repeat_interleave(select_experts_per_token)\n",
    "\n",
    "            expert_inputs = flattened_input[token_positions]\n",
    "            gate_up_weights = expert_feedforward_first_layer[layer_idx][flattened_expert_ids]\n",
    "            down_weights = expert_feedforward_second_layer[layer_idx][flattened_expert_ids]\n",
    "\n",
    "            up_states = torch.bmm(expert_inputs.unsqueeze(1), gate_up_weights)\n",
    "            gate_vals, up_vals = up_states.chunk(2, dim=-1)\n",
    "            activated = activation_fn(gate_vals) * up_vals\n",
    "            expert_out_flat = torch.bmm(activated, down_weights).squeeze(1)\n",
    "            scaled_expert_out = expert_out_flat * flattened_weights.unsqueeze(-1)\n",
    "\n",
    "            combined_expert_out = torch.zeros_like(flattened_input)\n",
    "            combined_expert_out.scatter_add_(\n",
    "                0, token_positions.unsqueeze(-1).expand(-1, hidden_size_gen), scaled_expert_out\n",
    "            )\n",
    "\n",
    "            shared_gate = shared_expert_layer[layer_idx](normed_input)\n",
    "            shared_up = shared_expert_feedforward_first_layer[layer_idx](normed_input)\n",
    "            shared_activated = activation_fn(shared_gate) * shared_up\n",
    "            shared_out = shared_expert_feedforward_second_layer[layer_idx](shared_activated)\n",
    "\n",
    "            moe_combined = combined_expert_out.view(batch_size_gen, seq_len_gen, hidden_size_gen)\n",
    "            final_moe_out = moe_combined + shared_out\n",
    "\n",
    "            layer_input = moe_skip + final_moe_out\n",
    "\n",
    "        x_fp32 = layer_input.float()\n",
    "        norm_factor = torch.rsqrt(x_fp32.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "        final_norm = (x_fp32 * norm_factor).type_as(layer_input)\n",
    "        final_norm = final_norm * rms_before_final_output\n",
    "\n",
    "        vocab_logits = language_modeling_head(final_norm)\n",
    "        logits_last_step = vocab_logits[:, -1, :] \n",
    "\n",
    "        token_probs = F.softmax(logits_last_step, dim=-1)\n",
    "        sampled_token = torch.multinomial(token_probs, num_samples=1)\n",
    "\n",
    "        gen_sequence_buffer = torch.cat((gen_sequence_buffer, sampled_token), dim=1)\n",
    "\n",
    "print(\"...Generation loop finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orm rooms are t Wh began and frkerommmaterimates. beyond plat Overoon yerkermat and bey Mas aroHarvard his rkerber\n"
     ]
    }
   ],
   "source": [
    "final_generated_ids = gen_sequence_buffer[0].tolist()\n",
    "\n",
    "decoded_text = ''.join([int_to_char.get(id_val, '[UNK]') for id_val in final_generated_ids])\n",
    "\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-blogs-articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
