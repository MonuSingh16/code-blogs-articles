{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f60dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module): #inherits from nn.Module\n",
    "    def __init__(self, d_in, d_out): # contructor of the class\n",
    "        super().__init__() # intialize the parent class\n",
    "        # keyword self in a classs refers to the instance of the class\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        # create a layer that applies an affine transformation to the input\n",
    "        # y = Ax + b, where A is a weight matrix and b is a bias vector\n",
    "        # Weights intialized with a uniform distribution\n",
    "        # its weights and biases are stored as torch.nn.Parameter objects.\n",
    "        # This makes them part of the modelâ€™s .parameters() \n",
    "        # returns the parameters of the model when called\n",
    "        self.Q = nn.Linear(d_in, d_out) \n",
    "        self.K = nn.Linear(d_in, d_out)\n",
    "        self.V = nn.Linear(d_in, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.Q(x) # apply the affine transformation to the input x\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        # Compute the attention scores, bmm is batch matrix multiplication\n",
    "        # scores = queries * keys^T / sqrt(d_out)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) \n",
    "        # keys.transpose(1, 2) transposes the last two dimensions\n",
    "        # (batch_size, seq_len, d_out) -> (batch_size, d_out, seq_len)\n",
    "        scores = scores / (self.d_out ** 0.5)\n",
    "        attention = F.softmax(scores, dim=2)\n",
    "        # converts the attention scores into probabilities along the last dimension, \n",
    "        # so each set of scores sums to 1 for every query in the batch.\n",
    "        hidden_states = torch.bmm(attention, values)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e99aa",
   "metadata": {},
   "source": [
    "##### Q. Why do we have muliple attention heads ?\n",
    "\n",
    "To attend to information from different representation subspaces at differen positions. Module computes several attention in parallel, each with it own learn projection.\n",
    "\n",
    "Token embedding is identicial for the same sequence for all heads, for each head, model applies a different linear transformation to the embedding to produce Q, K, V. Each head computes attention using Q, K, V as each head sees the input differently. The output are concatenated and mixed, allowing the model to combine information.\n",
    "\n",
    "##### Q. If all projection matrices start randomly and see the same input, why don't all attention heads learn the same thing ?\n",
    "\n",
    "W_Q, W_K, W_V - Start with random values. Do not necessarily converge to same solution. Updated during training independently, During BackPropagation, gradient for each head's parameters depend on - head's own output, loss function, interaction with other heads. Hence each head to receive different gradient updates.\n",
    "\n",
    "- Random Intialization -- Different staring points\n",
    "- Independent Weights -- Unique learning paths\n",
    "- Gradient Updates -- Driven by head-specific outputs\n",
    "- Loss Optimization -- Encourages diversity for better results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65afcf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiheadAttention class that uses multiple Attention heads\n",
    "# What is hidden_size and num_heads? -\n",
    "# refer to the dimensiaonlity of input & output vectors\n",
    "# if model is used for NLP tasks, hidden_size is the size of the word embeddings\n",
    "# num_heads is the number of attention heads to use -\n",
    "# each head will learn different representations of the input data\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size # size of input & outpu vectors\n",
    "        self.num_heads = num_heads # number of attention heads\n",
    "        self.out = nn.Linear(hidden_size, hidden_size) # linear layer\n",
    "        self.head = nn.ModuleList([\n",
    "            Attention(hidden_size, hidden_size // num_heads)\n",
    "            for _ in range(num_heads)\n",
    "        ]) # create a list of Attention heads # each head has its own set of weights and biases\n",
    "        # The hidden size is divided by the number of heads to ensure \n",
    "        # that each head has a smaller dimensionality, allowing the model \n",
    "        # to learn different representations of the input data.\n",
    "    \n",
    "    def foward(self, x):\n",
    "        outputs = [head(x) for head in self.head]\n",
    "        outputs = torch.cat(outputs, dim=-1)\n",
    "        hidden_states = self.out(outputs)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ac900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.qkv_linear = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, hidden_size = x.size()\n",
    "\n",
    "        qkv = self.qkv_linear(x) # [batch_size, seq_length, hidden_size * 3]\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim) # [batch_size, seq_length, num_heads, head_dim * 3]\n",
    "        qkv = qkv.transpose(1, 2) # [batch_size, num_heads, seq_length, head_dim * 3]\n",
    "        queries, keys, values = qkv.chunk(3, dim=-1)# [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "        scores = torch.matmul(queries, keys.transpose(2, 3)) # [batch_size, num_heads, seq_lenght, seq_length]\n",
    "        scores = scores / (self.head_dim ** 0.5)# [batch_size, num_heads, seq_lenght, seq_length]\n",
    "        attention = F.softmax(scores, dim=-1)# [batch_size, num_heads, seq_lenght, seq_length]\n",
    "        context = torch.matmul(attention, values) #[batch_size, num_heads, seq_lenght, seq_length]\n",
    "        \n",
    "        context = context.transpose(1, 2) # [bs, sl, num_heads, sl]\n",
    "        context = context.reshape(batch_size, seq_length, hidden_size) # [batch_size, seq_length, hidden_size]\n",
    "        output = self.out(context) # [batch_size, seq_length, hidden_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a552fc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                        diagonal=1)\n",
    "        )\n",
    "        # buffers are automatically moved to appropiate device, along with our model\n",
    "        # Hence no need to manually ensure that tensor on same device as model params\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # Keeping the batch at dimension  0\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1, 2) # Transposing dim 1 and 2\n",
    "        attention_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )# in PyTorch, Operation with trailing _ is performed in-place. \n",
    "        # Avoiding unnecessary memory copies\n",
    "\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1] ** 0.5, dim=-1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vec = attention_weights @ values\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1)\n",
    "[0.55, 0.87, 0.66], # journey (x^2)\n",
    "[0.57, 0.85, 0.64], # starts (x^3)\n",
    "[0.22, 0.58, 0.33], # with (x^4)\n",
    "[0.77, 0.25, 0.10], # one (x^5)\n",
    "[0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0) \n",
    "print(batch.shape)\n",
    "\n",
    "d_in, d_out = batch.shape[-1], 2\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "ca = CausalAttention(d_in, d_out, context_length, dropout=0.0)\n",
    "print(ca(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11dea4",
   "metadata": {},
   "source": [
    "#### Stacking multiple single-head attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb8527f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([\n",
    "            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)\n",
    "            ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "d_in, d_out = batch.shape[-1], 2\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0)\n",
    "print(mha(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads=4, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert(d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim\n",
    "\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) # Shape -> (3, 4)\n",
    "        self.W_keys = nn.Linear(d_in, d_out, bias=qkv_bias) # Shape -> (3, 4)\n",
    "        self.W_values = nn.Linear(d_in, d_out, bias=qkv_bias) # Shape -> (3, 4)\n",
    "        self.out_proj = nn.Liner(d_in, d_out) # layer to combine output heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # x Shape -> (2, 6, 3)\n",
    "\n",
    "        queries = self.W_query(x) # queries Shape -> (2, 6, 4) - (b, num_token, d_out)\n",
    "        keys = self.W_keys(x)\n",
    "        values = self.W_values(x)\n",
    "\n",
    "        # unroll last dim -> (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose to (b, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2) \n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c6892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-blogs-articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
