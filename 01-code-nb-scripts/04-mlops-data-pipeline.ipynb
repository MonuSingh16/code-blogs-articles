{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonuSingh16/code-blogs-articles/blob/main/04-mlops-data-pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==2.2.2 numpy==2.0.2 matplotlib==3.10.0 scikit-learn==1.6.1"
      ],
      "metadata": {
        "id": "j3LQ-wtBICrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90255e99-d223-4fb4-fbba-5a258a4cfdf8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy==2.0.2 in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib==3.10.0 in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.10.0) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "TqhmR4ctDzap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import logging, random\n",
        "import numpy as np\n",
        "\n",
        "# Colab working dirs\n",
        "ROOT = Path(\"/content\")\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "OUT_DIR = ROOT / \"output\"\n",
        "FIG_DIR = OUT_DIR / \"figs\"\n",
        "for p in [DATA_DIR, OUT_DIR, FIG_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reproducibility\n",
        "RNG_SEED = 42\n",
        "np.random.seed(RNG_SEED)\n",
        "random.seed(RNG_SEED)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")"
      ],
      "metadata": {
        "id": "MliHEnIUla5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "NwkPJH4cD4D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def _ts(s: str) -> pd.Timestamp:\n",
        "    return pd.to_datetime(s, utc=False, errors=\"coerce\")\n",
        "\n",
        "def save_json(obj, path: Path):\n",
        "    path.write_text(json.dumps(obj, indent=2, default=str), encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "jTmfUGQsleJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Generation"
      ],
      "metadata": {
        "id": "UMY6NUsED9od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def bootstrap_demo_data():\n",
        "    sales_csv = DATA_DIR / \"sales.csv\"\n",
        "    events_json = DATA_DIR / \"events.json\"\n",
        "    customers_db = DATA_DIR / \"customers.db\"\n",
        "\n",
        "    if sales_csv.exists() and events_json.exists() and customers_db.exists():\n",
        "        logging.info(\"Demo data already present. Skipping bootstrap.\")\n",
        "        return\n",
        "\n",
        "    logging.info(\"Bootstrapping demo data...\")\n",
        "    # Customers\n",
        "    n_customers = 400\n",
        "    countries = [\"IN\", \"US\", \"UK\", \"DE\", \"SG\"]\n",
        "    cities_by_country = {\n",
        "        \"IN\": [\"Delhi\", \"Bengaluru\", \"Mumbai\", \"Hyderabad\"],\n",
        "        \"US\": [\"NYC\", \"SF\", \"Austin\"],\n",
        "        \"UK\": [\"London\", \"Manchester\"],\n",
        "        \"DE\": [\"Berlin\", \"Munich\"],\n",
        "        \"SG\": [\"Singapore\"]\n",
        "    }\n",
        "\n",
        "    start_date = datetime(2023, 9, 1)\n",
        "    end_date = datetime(2024, 9, 1)\n",
        "    days = (end_date - start_date).days\n",
        "\n",
        "    cust_rows = []\n",
        "    for cid in range(1001, 1001 + n_customers):\n",
        "        country = random.choice(countries)\n",
        "        city = random.choice(cities_by_country[country])\n",
        "        signup = start_date + timedelta(days=int(np.random.uniform(0, max(1, days*0.6))))\n",
        "        cust_rows.append({\n",
        "            \"customer_id\": cid,\n",
        "            \"country\": country,\n",
        "            \"city\": city,\n",
        "            \"signup_date\": signup.date().isoformat()\n",
        "        })\n",
        "\n",
        "    # SQLite customers table\n",
        "    conn = sqlite3.connect(customers_db)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS customers (\n",
        "            customer_id INTEGER PRIMARY KEY,\n",
        "            country TEXT,\n",
        "            city TEXT,\n",
        "            signup_date TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "    cur.executemany(\n",
        "        \"INSERT OR REPLACE INTO customers (customer_id, country, city, signup_date) VALUES (?, ?, ?, ?)\",\n",
        "        [(r[\"customer_id\"], r[\"country\"], r[\"city\"], r[\"signup_date\"]) for r in cust_rows]\n",
        "    )\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    # Sales CSV with realistic noise/outliers\n",
        "    sale_rows = []\n",
        "    sale_id = 1\n",
        "    for r in cust_rows:\n",
        "        base_lambda = 10\n",
        "        signup_dt = _ts(r[\"signup_date\"])\n",
        "        active_days = max(1, (end_date - max(start_date, signup_dt.to_pydatetime())).days)\n",
        "        expected = max(0.5, base_lambda * (active_days / days))\n",
        "        n = np.random.poisson(lam=expected)\n",
        "\n",
        "        for _ in range(n):\n",
        "            ts = start_date + timedelta(days=int(np.random.uniform(0, days)))\n",
        "            if ts.date() < signup_dt.date():\n",
        "                continue\n",
        "            amt = np.random.lognormal(mean=5.0, sigma=0.6)\n",
        "            if np.random.rand() < 0.02:\n",
        "                amt = np.nan\n",
        "            if np.random.rand() < 0.01:\n",
        "                amt = -abs(amt)\n",
        "\n",
        "            sale_rows.append({\n",
        "                \"sale_id\": sale_id,\n",
        "                \"customer_id\": r[\"customer_id\"],\n",
        "                \"amount\": amt,\n",
        "                \"ts\": ts.isoformat(timespec=\"seconds\")\n",
        "            })\n",
        "            sale_id += 1\n",
        "\n",
        "    pd.DataFrame(sale_rows).sort_values(\"ts\").to_csv(sales_csv, index=False)\n",
        "\n",
        "    # Events JSON\n",
        "    event_types = [\"login\", \"view\", \"cart_add\", \"wishlist\"]\n",
        "    ev_rows = []\n",
        "    ev_id = 1\n",
        "    for r in cust_rows:\n",
        "        m = np.random.poisson(lam=5.0)\n",
        "        for _ in range(m):\n",
        "            ts = start_date + timedelta(days=int(np.random.uniform(0, days)))\n",
        "            if ts.date() < _ts(r[\"signup_date\"]).date():\n",
        "                continue\n",
        "            ev_rows.append({\n",
        "                \"event_id\": ev_id,\n",
        "                \"customer_id\": r[\"customer_id\"],\n",
        "                \"event_type\": random.choice(event_types),\n",
        "                \"ts\": ts.isoformat(timespec=\"seconds\")\n",
        "            })\n",
        "            ev_id += 1\n",
        "\n",
        "    save_json(ev_rows, events_json)\n",
        "    logging.info(\"Bootstrap complete\")\n",
        "\n",
        "bootstrap_demo_data()"
      ],
      "metadata": {
        "id": "-RmE_kvNlrkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extraction Phase - I"
      ],
      "metadata": {
        "id": "3ncGopWbEDov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sales_csv(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    logging.info(\"Extracted sales CSV: %s rows, %s cols\", *df.shape)\n",
        "    return df\n",
        "\n",
        "def extract_events_json(path: Path) -> pd.DataFrame:\n",
        "    data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "    df = pd.json_normalize(data)\n",
        "    logging.info(\"Extracted events JSON: %s rows, %s cols\", *df.shape)\n",
        "    return df\n",
        "\n",
        "def extract_customers_sqlite(path: Path) -> pd.DataFrame:\n",
        "    conn = sqlite3.connect(path)\n",
        "    df = pd.read_sql(\"SELECT * FROM customers\", conn)\n",
        "    conn.close()\n",
        "    logging.info(\"Extracted customers SQLite: %s rows, %s cols\", *df.shape)\n",
        "    return df\n",
        "\n",
        "sales = extract_sales_csv(DATA_DIR / \"sales.csv\")\n",
        "events = extract_events_json(DATA_DIR / \"events.json\")\n",
        "customers = extract_customers_sqlite(DATA_DIR / \"customers.db\")\n",
        "\n",
        "sales.head(3), events.head(3), customers.head(3)"
      ],
      "metadata": {
        "id": "_ucnDEjzlyo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation"
      ],
      "metadata": {
        "id": "hpWXi-itE8sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_sales(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in [\"sale_id\", \"customer_id\", \"amount\", \"ts\"]:\n",
        "        if c not in df.columns:\n",
        "            raise ValueError(f\"sales missing required column: {c}\")\n",
        "    df[\"sale_id\"] = pd.to_numeric(df[\"sale_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    df[\"customer_id\"] = pd.to_numeric(df[\"customer_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
        "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\")\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.dropna(subset=[\"sale_id\", \"customer_id\", \"ts\"]).reset_index(drop=True)\n",
        "\n",
        "    # Negative / zero -> NaN, cap extreme outliers\n",
        "    df.loc[df[\"amount\"] <= 0, \"amount\"] = np.nan\n",
        "    cap = df[\"amount\"].quantile(0.95) * 5 if df[\"amount\"].notna().sum() > 0 else np.nan\n",
        "    df.loc[df[\"amount\"] > cap, \"amount\"] = cap\n",
        "\n",
        "    # Fill amounts: customer median -> global median\n",
        "    df[\"amount\"] = df.groupby(\"customer_id\")[\"amount\"].transform(lambda s: s.fillna(s.median()))\n",
        "    df[\"amount\"] = df[\"amount\"].fillna(df[\"amount\"].median())\n",
        "\n",
        "    # Deduplicate by latest ts per sale_id\n",
        "    df = df.sort_values([\"sale_id\", \"ts\"]).drop_duplicates(subset=[\"sale_id\"], keep=\"last\")\n",
        "    logging.info(\"Validated sales: dropped %d bad/dup rows\", before - len(df))\n",
        "    return df\n",
        "\n",
        "def validate_events(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    req = {\"event_id\", \"customer_id\", \"event_type\", \"ts\"}\n",
        "    if not req.issubset(df.columns):\n",
        "        raise ValueError(f\"events missing required columns: {req - set(df.columns)}\")\n",
        "    df = df.copy()\n",
        "    df[\"event_id\"] = pd.to_numeric(df[\"event_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    df[\"customer_id\"] = pd.to_numeric(df[\"customer_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    df[\"event_type\"] = df[\"event_type\"].astype(\"string\")\n",
        "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\")\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.dropna(subset=[\"event_id\", \"customer_id\", \"event_type\", \"ts\"])\n",
        "    df = df.sort_values([\"event_id\", \"ts\"]).drop_duplicates(subset=[\"event_id\"], keep=\"last\")\n",
        "    logging.info(\"Validated events: dropped %d bad/dup rows\", before - len(df))\n",
        "    return df\n",
        "\n",
        "def validate_customers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    req = {\"customer_id\", \"country\", \"city\", \"signup_date\"}\n",
        "    if not req.issubset(df.columns):\n",
        "        raise ValueError(f\"customers missing required columns: {req - set(df.columns)}\")\n",
        "    df = df.copy()\n",
        "    df[\"customer_id\"] = pd.to_numeric(df[\"customer_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    df[\"country\"] = df[\"country\"].astype(\"string\")\n",
        "    df[\"city\"] = df[\"city\"].astype(\"string\")\n",
        "    df[\"signup_date\"] = pd.to_datetime(df[\"signup_date\"], errors=\"coerce\").dt.date\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.dropna(subset=[\"customer_id\", \"country\", \"city\", \"signup_date\"])\n",
        "    df = df.drop_duplicates(subset=[\"customer_id\"], keep=\"last\")\n",
        "    logging.info(\"Validated customers: dropped %d bad/dup rows\", before - len(df))\n",
        "    return df\n",
        "\n",
        "sales_v = validate_sales(sales)\n",
        "events_v = validate_events(events)\n",
        "customers_v = validate_customers(customers)\n",
        "\n",
        "sales_v.head(3), events_v.head(3), customers_v.head(3)"
      ],
      "metadata": {
        "id": "L0GQIaNLl1z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform Phase - I"
      ],
      "metadata": {
        "id": "hUT1F8pPFDef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_make_features(sales: pd.DataFrame,\n",
        "                            customers: pd.DataFrame,\n",
        "                            cutoff: pd.Timestamp,\n",
        "                            obs_days: int,\n",
        "                            label_days: int):\n",
        "    sales = sales.copy()\n",
        "    customers = customers.copy()\n",
        "\n",
        "    obs_start = cutoff - timedelta(days=obs_days)\n",
        "    label_end = cutoff + timedelta(days=label_days)\n",
        "\n",
        "    sales[\"ts\"] = pd.to_datetime(sales[\"ts\"])\n",
        "    obs_mask = (sales[\"ts\"] >= obs_start) & (sales[\"ts\"] < cutoff)\n",
        "    label_mask = (sales[\"ts\"] >= cutoff) & (sales[\"ts\"] < label_end)\n",
        "\n",
        "    sales_obs = sales.loc[obs_mask].copy()\n",
        "    sales_label = sales.loc[label_mask].copy()\n",
        "\n",
        "    # Daily time series for viz\n",
        "    daily_obs = (sales_obs.assign(day=sales_obs[\"ts\"].dt.date)\n",
        "                 .groupby(\"day\", as_index=False)[\"amount\"].sum()\n",
        "                 .rename(columns={\"amount\": \"daily_sales\"}))\n",
        "\n",
        "    # RFM-like features\n",
        "    last_ts = sales_obs.groupby(\"customer_id\")[\"ts\"].max().to_frame(\"last_ts\").reset_index()\n",
        "    last_ts[\"recency_days\"] = (cutoff - last_ts[\"last_ts\"]).dt.days\n",
        "\n",
        "    agg = sales_obs.groupby(\"customer_id\").agg(\n",
        "        freq=(\"sale_id\", \"count\"),\n",
        "        monetary=(\"amount\", \"sum\"),\n",
        "        avg_amount=(\"amount\", \"mean\"),\n",
        "        max_amount=(\"amount\", \"max\")\n",
        "    ).reset_index()\n",
        "\n",
        "    def avg_gap(group):\n",
        "        g = group.sort_values(\"ts\")[\"ts\"].values\n",
        "        if len(g) < 2:\n",
        "            return np.nan\n",
        "        diffs = np.diff(g).astype(\"timedelta64[D]\").astype(float)\n",
        "        return float(np.mean(diffs))\n",
        "\n",
        "    gaps = sales_obs.groupby(\"customer_id\", group_keys=False).apply(avg_gap, include_groups=False).to_frame(\"avg_gap_days\").reset_index()\n",
        "\n",
        "    feats = customers[[\"customer_id\", \"country\", \"city\", \"signup_date\"]].merge(agg, on=\"customer_id\", how=\"left\")\n",
        "    feats = feats.merge(last_ts[[\"customer_id\", \"recency_days\"]], on=\"customer_id\", how=\"left\")\n",
        "    feats = feats.merge(gaps, on=\"customer_id\", how=\"left\")\n",
        "\n",
        "    feats[\"freq\"] = feats[\"freq\"].fillna(0).astype(int)\n",
        "    for c in [\"monetary\", \"avg_amount\", \"max_amount\", \"recency_days\", \"avg_gap_days\"]:\n",
        "        feats[c] = feats[c].astype(float)\n",
        "\n",
        "    feats[\"tenure_days\"] = (pd.to_datetime(cutoff.date()) - pd.to_datetime(feats[\"signup_date\"])).dt.days.clip(lower=0)\n",
        "\n",
        "    has_future_purchase = sales_label.groupby(\"customer_id\")[\"sale_id\"].count().reindex(feats[\"customer_id\"]).fillna(0)\n",
        "    feats[\"label_churn\"] = (has_future_purchase.values == 0).astype(int)\n",
        "\n",
        "    feats = feats[[\n",
        "        \"customer_id\", \"country\", \"city\", \"tenure_days\",\n",
        "        \"freq\", \"monetary\", \"avg_amount\", \"max_amount\",\n",
        "        \"recency_days\", \"avg_gap_days\", \"label_churn\"\n",
        "    ]]\n",
        "\n",
        "    return feats, daily_obs, sales_obs\n",
        "\n",
        "default_cutoff = pd.to_datetime(sales_v[\"ts\"].quantile(0.75)).normalize()\n",
        "obs_days, label_days = 200, 60\n",
        "\n",
        "feats, daily_obs, sales_obs = transform_make_features(\n",
        "    sales=sales_v, customers=customers_v,\n",
        "    cutoff=default_cutoff, obs_days=obs_days, label_days=label_days\n",
        ")\n",
        "\n",
        "default_cutoff, feats.shape, sales_obs.shape, daily_obs.shape"
      ],
      "metadata": {
        "id": "rNwcQMrSl-t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visual Analysis"
      ],
      "metadata": {
        "id": "v78SFgZwFQwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_missingness(df: pd.DataFrame, title: str, out_path: Path):\n",
        "    miss = df.isna().mean()\n",
        "    miss = miss[miss > 0].sort_values(ascending=False)  # only plot columns with missing\n",
        "    if miss.empty:\n",
        "        print(f\"No missing values found in: {title}\")\n",
        "        return\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    miss.plot(kind=\"bar\")\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"Fraction Missing\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_amount_hist(sales_obs: pd.DataFrame, out_path: Path):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.hist(sales_obs[\"amount\"].dropna(), bins=40)\n",
        "    plt.title(\"Amount Distribution (Observation Window)\")\n",
        "    plt.xlabel(\"Amount\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_daily_sales(daily_obs: pd.DataFrame, out_path: Path):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(pd.to_datetime(daily_obs[\"day\"]), daily_obs[\"daily_sales\"])\n",
        "    plt.title(\"Daily Sales (Observation Window)\")\n",
        "    plt.xlabel(\"Day\")\n",
        "    plt.ylabel(\"Sales\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_corr_heatmap(feats: pd.DataFrame, out_path: Path):\n",
        "    numeric = feats.select_dtypes(include=[np.number])\n",
        "    if numeric.shape[1] < 2:\n",
        "        return\n",
        "    corr = numeric.corr()\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.imshow(corr, aspect=\"auto\")\n",
        "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(len(corr.index)), corr.index)\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Feature Correlation (Numeric)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "# Generate and show inline\n",
        "plot_missingness(sales_v, \"Missingness: Sales (All)\", FIG_DIR / \"missing_sales.png\")\n",
        "plot_missingness(customers_v, \"Missingness: Customers\", FIG_DIR / \"missing_customers.png\")\n",
        "plot_amount_hist(sales_obs, FIG_DIR / \"amount_hist_obs.png\")\n",
        "plot_daily_sales(daily_obs, FIG_DIR / \"daily_sales_obs.png\")\n",
        "plot_corr_heatmap(feats.drop(columns=[\"customer_id\"]), FIG_DIR / \"feature_corr.png\")\n",
        "\n",
        "from IPython.display import Image, display\n",
        "for f in FIG_DIR.glob(\"*.png\"):\n",
        "    display(Image(f))"
      ],
      "metadata": {
        "id": "ZzkUgb5ZmCFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Phase - I"
      ],
      "metadata": {
        "id": "1q1GWDlYGC8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dump_outputs(feats: pd.DataFrame, sales_obs: pd.DataFrame, daily_obs: pd.DataFrame,\n",
        "                 cutoff: pd.Timestamp, obs_days: int, label_days: int):\n",
        "    feats.to_csv(OUT_DIR / \"features.csv\", index=False)\n",
        "    feats.to_parquet(OUT_DIR / \"features.parquet\", index=False)\n",
        "    feats.to_json(OUT_DIR / \"features.json\", orient=\"records\", indent=2)\n",
        "\n",
        "    sales_obs.to_csv(OUT_DIR / \"sales_observation.csv\", index=False)\n",
        "    sales_obs.to_parquet(OUT_DIR / \"sales_observation.parquet\", index=False)\n",
        "    sales_obs.to_json(OUT_DIR / \"sales_observation.json\", orient=\"records\", indent=2)\n",
        "\n",
        "    daily_obs.to_csv(OUT_DIR / \"daily_sales.csv\", index=False)\n",
        "\n",
        "    data_dict = {\n",
        "        \"features\": {\n",
        "            \"customer_id\": \"Unique customer key\",\n",
        "            \"country\": \"Country code\",\n",
        "            \"city\": \"City name\",\n",
        "            \"tenure_days\": \"Days since signup until cutoff\",\n",
        "            \"freq\": \"Purchase count in observation window\",\n",
        "            \"monetary\": \"Total spend in observation window\",\n",
        "            \"avg_amount\": \"Average spend per purchase (obs window)\",\n",
        "            \"max_amount\": \"Max spend in observation window\",\n",
        "            \"recency_days\": \"Days since last purchase at cutoff\",\n",
        "            \"avg_gap_days\": \"Avg days between purchases (obs window)\",\n",
        "            \"label_churn\": \"1 if NO purchase in label window; else 0\"\n",
        "        }\n",
        "    }\n",
        "    save_json(data_dict, OUT_DIR / \"data_dictionary.json\")\n",
        "    meta = {\n",
        "        \"run_timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "        \"rng_seed\": RNG_SEED,\n",
        "        \"params\": {\n",
        "            \"cutoff\": str(cutoff.date()),\n",
        "            \"obs_days\": obs_days,\n",
        "            \"label_days\": label_days\n",
        "        },\n",
        "        \"rows\": {\n",
        "            \"features\": int(len(feats)),\n",
        "            \"sales_obs\": int(len(sales_obs)),\n",
        "            \"daily_obs\": int(len(daily_obs))\n",
        "        }\n",
        "    }\n",
        "    save_json(meta, OUT_DIR / \"run_metadata.json\")\n",
        "\n",
        "    qa = {\n",
        "        \"n_customers_total\": int(customers_v[\"customer_id\"].nunique()),\n",
        "        \"n_customers_with_obs_purchases\": int(sales_obs[\"customer_id\"].nunique()),\n",
        "        \"n_features_rows\": int(len(feats)),\n",
        "        \"churn_rate\": float(feats[\"label_churn\"].mean()),\n",
        "        \"obs_window_start\": str((cutoff - timedelta(days=obs_days)).date()),\n",
        "        \"cutoff\": str(cutoff.date()),\n",
        "        \"label_window_end\": str((cutoff + timedelta(days=label_days)).date())\n",
        "    }\n",
        "    save_json(qa, OUT_DIR / \"qa_report.json\")\n",
        "    return qa\n",
        "\n",
        "qa = dump_outputs(feats, sales_obs, daily_obs, default_cutoff, obs_days, label_days)\n",
        "qa"
      ],
      "metadata": {
        "id": "-Z3CdSRgmF3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract - II and EDA/Checks"
      ],
      "metadata": {
        "id": "4u4LSgLSGKjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the saved features file\n",
        "# df = pd.read_csv(\"output/features.csv\")\n",
        "# df = pd.read_parquet(\"output/features.parquet\")\n",
        "df = pd.read_json(\"output/features.json\")\n",
        "\n",
        "# Quick EDA to confirm shape and missingness\n",
        "print(df.shape)\n",
        "print(df.isnull().sum())\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "id": "OG53wz8SmzHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load - II"
      ],
      "metadata": {
        "id": "cN9BdU57TBMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"data.csv\", index=False)"
      ],
      "metadata": {
        "id": "viG1H0PoS9JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform Phase -II"
      ],
      "metadata": {
        "id": "q1Yf-wBAGd5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load data\n",
        "_df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = _df.drop(columns=[\"label_churn\", \"customer_id\"])\n",
        "y = _df[\"label_churn\"]\n",
        "\n",
        "# Split BEFORE fitting anything (avoid leakage)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Infer column types from TRAIN ONLY\n",
        "numeric_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "categorical_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "# Pipelines\n",
        "numeric_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),  # dense\n",
        "])\n",
        "\n",
        "# Build ColumnTransformer (skip empty groups gracefully)\n",
        "transformers = []\n",
        "if numeric_cols:\n",
        "    transformers.append((\"num\", numeric_pipeline, numeric_cols))\n",
        "if categorical_cols:\n",
        "    transformers.append((\"cat\", categorical_pipeline, categorical_cols))\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers)\n",
        "\n",
        "# Fit on train, transform both\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_valid_processed = preprocessor.transform(X_valid)\n",
        "\n",
        "# Feature names\n",
        "num_features = numeric_cols\n",
        "cat_features = []\n",
        "if categorical_cols:\n",
        "    cat_features = preprocessor.named_transformers_[\"cat\"][\"encoder\"]\\\n",
        "                               .get_feature_names_out(categorical_cols).tolist()\n",
        "all_features = num_features + cat_features\n",
        "\n",
        "# Back to DataFrames with aligned indices\n",
        "X_train_df = pd.DataFrame(X_train_processed, columns=all_features, index=X_train.index).reset_index(drop=True)\n",
        "X_valid_df = pd.DataFrame(X_valid_processed, columns=all_features, index=X_valid.index).reset_index(drop=True)\n",
        "\n",
        "# Reattach target\n",
        "train_final_df = pd.concat([X_train_df, y_train.reset_index(drop=True)], axis=1)\n",
        "valid_final_df = pd.concat([X_valid_df, y_valid.reset_index(drop=True)], axis=1)"
      ],
      "metadata": {
        "id": "GQ7M6tTad9aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final datasets (Load - III)\n",
        "\n",
        "# OR\n",
        "\n",
        "# CAN BE USED FOR MODEL TRAINING AND VALIDATION DIRECTLY FROM HERE"
      ],
      "metadata": {
        "id": "W7A-nWkqz0Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Rb5RV9v8Q5d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}