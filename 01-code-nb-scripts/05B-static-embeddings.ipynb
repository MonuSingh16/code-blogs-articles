{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8addfa",
   "metadata": {},
   "source": [
    "#### 01. Static Embeddings - Word2Vec\n",
    "\n",
    "##### Skip-gram\n",
    "\n",
    "- Goal: Predict the context words given a target (center) word.\n",
    "\n",
    "- How it works: For each word in a sentence, the model tries to guess the words that appear in its surrounding window.\n",
    "\n",
    "- Example: Sentence: \"He sat by the river bank\"\n",
    "If the center word is \"bank\" and window=2, the context words are [\"the\", \"river\"].\n",
    "\n",
    "- Training pairs:\n",
    "    - Input: \"bank\" \n",
    "    - Outputs: \"the\", \"river\"\n",
    "\n",
    "- Best for: Learning good representations for rare words.\n",
    "\n",
    "##### CBOW (Continuous Bag of Words)\n",
    "\n",
    "- Goal: Predict the target (center) word given its context words.\n",
    "\n",
    "- How it works: For each word in a sentence, the model tries to guess the word in the middle from the words around it.\n",
    "\n",
    "- Example: Sentence: \"He sat by the river bank\"\n",
    "If the context is [\"the\", \"river\"], the model tries to predict \"bank\".\n",
    "\n",
    "- Training pairs:\n",
    "    - Input: [\"the\", \"river\"]\n",
    "    - Output: \"bank\"\n",
    "\n",
    "- Best for: Faster training, works well with frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc9413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f98d1438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vector for 'bank':\n",
      " [-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
      " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
      " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
      "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
      "  9.9641159e-03  1.8466286e-02]\n",
      "Word2Vec : Bank's Word Vector Shape (50,)\n",
      "Similarity between 'bank' and 'river': -0.012591083\n",
      "Similarity between 'bank' and 'money': 0.13204393\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import Word2Vec from gensim\n",
    "# Gensim is an open-source Python library for unsupervised topic modeling \n",
    "# and natural language processing, with a focus on efficient, scalable algorithms \n",
    "# for learning vector representations of text.\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Basic tokenizer: splits sentences into lowercase words\n",
    "def simple_tokenizer(text):\n",
    "    # Lowercase and split on spaces (very basic, for demo only)\n",
    "    return text.lower().replace('.', '').split()\n",
    "\n",
    "# Example sentences with the word 'bank' in different contexts\n",
    "raw_sentences = [\n",
    "    \"He sat by the river bank.\",\n",
    "    \"She deposited money in the bank.\",\n",
    "    \"The bank was closed on Sunday.\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [simple_tokenizer(sent) for sent in raw_sentences]\n",
    "# [He, Sat, by, the]\n",
    "\n",
    "# Train Word2Vec model\n",
    "# vector_size: dimension of the embedding vectors (higher = more expressive, but needs more data)\n",
    "# min_count: ignore words with total frequency lower than this (set to 1 to include all words in this tiny corpus)\n",
    "# window: maximum distance between the current and predicted word within a sentence (context window size)\n",
    "w2v_model = Word2Vec(tokenized_sentences, vector_size=50, min_count=1, window=3)\n",
    "# This is Skip-gram (sg=1) by default and sg=0 for CBOW\n",
    "\n",
    "# Get embedding for 'bank'\n",
    "# The vector for 'bank' will be the same regardless of which sentence/context it appears in\n",
    "print(\"Word2Vec vector for 'bank':\\n\", w2v_model.wv['bank'])\n",
    "print(\"Word2Vec : Bank's Word Vector Shape\", w2v_model.wv['bank'].shape)\n",
    "\n",
    "words = list(w2v_model.wv.index_to_key)  # List all words in the vocabulary\n",
    "similar = w2v_model.wv.most_similar('bank')  # Find words most similar to 'bank'\n",
    "\n",
    "print(\"Similarity between 'bank' and 'river':\", w2v_model.wv.similarity('bank', 'river'))\n",
    "print(\"Similarity between 'bank' and 'money':\", w2v_model.wv.similarity('bank', 'money'))\n",
    "\n",
    "# NOTE:\n",
    "\n",
    "# - The model uses a sliding window (set by the window parameter) to look at neighboring words \n",
    "# - and learns that words appearing in similar contexts should have similar vectors.\n",
    "\n",
    "# - bank' has the same embedding in both \"river bank\" and \"money bank\" contexts.\n",
    "# - the word \"bank\" appears in both \"river bank\" and \"money bank\" contexts. \n",
    "# - The model tries to capture both, but since static embeddings can only assign one vector \n",
    "# - per word, \"bank\" gets a single vector that is an average of all its contexts.\n",
    "\n",
    "# - This is a limitation of static embeddings:\n",
    "# - they cannot distinguish between different meanings (senses) of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b57d7",
   "metadata": {},
   "source": [
    "#### 02. Static Embeddings - FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23872ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText vector for 'bank':\n",
      " [-0.02681367 -0.00070708  0.01805622  0.00670038 -0.00706677 -0.01905808\n",
      "  0.01269603  0.00208769  0.00945567 -0.02328101]\n",
      "FastText vector for OOV word 'banking':\n",
      " [ 0.00135347 -0.00299125  0.00331595  0.01845975  0.0230606   0.00761079\n",
      "  0.00130226  0.00841182  0.00700071 -0.01047717]\n",
      "FastText vector for OOV word 'bankzzz':\n",
      " [-0.02014219  0.00406886 -0.00836108  0.01079835  0.01020753  0.00953312\n",
      "  0.01484188 -0.01233316  0.0123117  -0.02921248]\n",
      "Similarity between 'bank' and 'banking': 0.112472326\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Train FastText model on your tokenized sentences\n",
    "ft_model = FastText(tokenized_sentences, vector_size=10, min_count=1, window=3)\n",
    "\n",
    "# Get embedding for a known word\n",
    "# Each word is broken into character n-grams (e.g., \"bank\" â†’ <ba, ban, ank, nk>, etc.).\n",
    "# The word vector is the sum (or average) of its n-gram vectors.\n",
    "# Training is similar to Word2Vec (CBOW or Skip-gram), but on subwords.\n",
    "print(\"FastText vector for 'bank':\\n\", ft_model.wv['bank'])\n",
    "\n",
    "# FastText can handle OOV (out-of-vocabulary) words using subword information\n",
    "print(\"FastText vector for OOV word 'banking':\\n\", ft_model.wv['banking'])\n",
    "\n",
    "# Compare with a nonsense word (still gets a vector!)\n",
    "print(\"FastText vector for OOV word 'bankzzz':\\n\", ft_model.wv['bankzzz'])\n",
    "\n",
    "# Demo: Similarity between 'bank' and 'banking'\n",
    "print(\"Similarity between 'bank' and 'banking':\", ft_model.wv.similarity('bank', 'banking'))\n",
    "\n",
    "# NOTE:\n",
    "# - FastText is especially useful when you expect to encounter new words,\n",
    "# - rare words, or work with morphologically rich languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79612057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-blogs-articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
