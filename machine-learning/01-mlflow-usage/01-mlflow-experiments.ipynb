{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a97e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Motivation\n",
    "# 1.1 - Often uses spreadhset to record parameter configs & data sources\n",
    "# 1.2 Inefficient Collaborations & Standardize Practices\n",
    "\n",
    "## MLflow Components\n",
    "# 1. Tracking - Experiments (code, data, model, configs and results) & Comparing for model selection\n",
    "# 2. Projects - Packaging code, make them reproducible on any platform\n",
    "# 3. Models - Deploying ML Models in diverse env; Model build of various libs\n",
    "# 4. Models Registry - Manage, organize, version and track ML Models & metadata\n",
    "# 5. Deploymnet for LLMs - Usage & Management of various LLM Providers\n",
    "# 6. LLM Evaluate - Evaluating LLMs and the prompt\n",
    "\n",
    "## MLflow Tracking\n",
    "# Experiment Models by training them on various hyperparameter, testing new features, more data and using different algos. Hence So Many combinations\n",
    "# Reproducibility - By Logging & recording all params, code versions and dependencies used\n",
    "# Experiment is a collection of runs. Where each run represents the model training. Group together related runs. Start a experiment, a dedicated space for it, allowing us to track & compare runs within that experiment easily\n",
    "# Run is a single execution of ML workflow within a specific experiment. Have all details of that execution, code, data, params, metrics & artifacts\n",
    "# MLflow log various details of a run.\n",
    "\n",
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-blogs-articles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
