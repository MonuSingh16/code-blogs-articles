{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2072c8f1",
   "metadata": {},
   "source": [
    "# Daily Dose of Data Science\n",
    "\n",
    "This notebook accompanies the exercise for Bagging article.\n",
    "\n",
    "Read the full blog here: [Why Bagging is So Ridiculously Effective At Variance Reduction?](https://www.dailydoseofds.com/why-bagging-is-so-ridiculously-effective-at-variance-reduction)\n",
    "\n",
    "Author: Avi Chawla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c717aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:15:13.560677Z",
     "start_time": "2023-10-11T11:15:13.551779Z"
    }
   },
   "source": [
    "## You are supposed to write your solution in between \"YOUR CODE STARTS HERE\" and \"YOUR CODE ENDS HERE\" in the sampling_function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1785c8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fafd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:15:32.261105Z",
     "start_time": "2023-10-11T11:15:31.035070Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813adf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:15:32.266754Z",
     "start_time": "2023-10-11T11:15:32.262637Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = ['Comic Sans MS', 'sans-serif']\n",
    "\n",
    "colors = ['#fe7c73', '#2471A3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8bb56",
   "metadata": {},
   "source": [
    "# Bagging Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a78896",
   "metadata": {},
   "source": [
    "## Feature subsetting for each tree:\n",
    "- In a typical random forest, Every tree gets to see all the features in the training set.\n",
    "- Here, we shall create a tree only on a subset of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416c139",
   "metadata": {},
   "source": [
    "### Define bootstrapping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff838f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T10:50:39.629123Z",
     "start_time": "2023-10-11T10:50:39.613051Z"
    }
   },
   "outputs": [],
   "source": [
    "def sampling_function(df, row_sample_ratio, feature_sample_ratio):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function accepts the following arguments:\n",
    "    \n",
    "    1. dataframe df of size (n,d), where:\n",
    "    -- n: number of training samples in the entire training set\n",
    "    -- d: number of dimensions in the entire training set\n",
    "    \n",
    "    2. row_sample_ratio: The fraction of rows to sample before duplicating during sample bootstrapping.\n",
    "    \n",
    "    3. feature_sample_ratio: The fraction of columns to sample before duplicating during column bootstrapping.\n",
    "    \n",
    "    This function returns another dataframe of size (n,d) which has:\n",
    "    - some duplicated rows\n",
    "    - some duplicated columns\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # define total samples and columns\n",
    "    total_samples, total_columns = df.shape\n",
    "    \n",
    "    # COLUMN SAMPLING:\n",
    "    \n",
    "    initial_column_size = int(feature_sample_ratio*total_columns)\n",
    "    \n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    \n",
    "    # randomly sample 'initial_column_size' number of columns from 'df' EXCEPT FOR THE Y COLUMN\n",
    "    initial_column_sample_df = \n",
    "    \n",
    "    remaining_columns = total_columns - initial_column_size\n",
    "    \n",
    "    new_columns = pd.DataFrame()\n",
    "    for _ in range(remaining_columns):\n",
    "        \n",
    "        # randomly sample one column from 'initial_column_sample_df'\n",
    "        sampled_column = \n",
    "        \n",
    "        # add it to 'new_columns' df with its column name\n",
    "        new_columns[] = \n",
    "        \n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "        \n",
    "    column_sampled_df = pd.concat((initial_column_sample_df, new_columns), axis = 1)\n",
    "    \n",
    "    # ROW SAMPLING\n",
    "    \n",
    "    # number of rows to sample\n",
    "    initial_sample_size = int(row_sample_ratio*total_samples)\n",
    "    \n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    \n",
    "    # randomly sample 'initial_sample_size' number of columns from 'column_sampled_df'\n",
    "    initial_row_sample_df = \n",
    "    \n",
    "    new_rows = pd.DataFrame()\n",
    "    \n",
    "    remaining_rows = final_size - initial_sample_size\n",
    "    for i in range(remaining_rows):\n",
    "        \n",
    "        # randomly sample one row from 'column_sampled_df'\n",
    "        sampled_row = \n",
    "        \n",
    "        # add it to 'new_rows' df\n",
    "        new_rows = pd.concat((new_rows, sampled_row))\n",
    "        \n",
    "    bootstrapped_dataset = pd.concat((initial_row_sample_df, new_rows))\n",
    "    return bootstrapped_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f4abe",
   "metadata": {},
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c067be8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T10:50:56.578757Z",
     "start_time": "2023-10-11T10:50:56.563531Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def predict(models, test_data):\n",
    "    # Assuming you have test data 'X_test' as a DataFrame\n",
    "    predictions = []\n",
    "\n",
    "    # Make predictions using each tree in 'all_trees'\n",
    "    for model in models:\n",
    "        model_predictions = model.predict(test_data)\n",
    "        predictions.append(model_predictions)\n",
    "\n",
    "    # Perform majority vote to aggregate predictions\n",
    "    ensemble_predictions = []\n",
    "    for i in range(len(test_data)):\n",
    "        ensemble_prediction = Counter([prediction[i] for prediction in predictions]).most_common(1)[0][0]\n",
    "        ensemble_predictions.append(ensemble_prediction)\n",
    "\n",
    "    ensemble_predictions = np.array(ensemble_predictions)\n",
    "    \n",
    "    return ensemble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c521900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:34:41.723751Z",
     "start_time": "2023-10-11T11:34:41.581530Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate some synthetic data to train the decision tree on\n",
    "X, y = make_classification(\n",
    "    n_samples=1200, \n",
    "    n_features=2, \n",
    "    n_clusters_per_class=1,\n",
    "    n_informative=2, class_sep=0.5,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    random_state=21\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=[colors[i] for i in y])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "plt.show()\n",
    "\n",
    "X_train, y_train = X[:1000], y[:1000]\n",
    "X_test, y_test = X[1000:], y[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46bbc9b",
   "metadata": {},
   "source": [
    "### Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953239f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T10:56:56.392851Z",
     "start_time": "2023-10-11T10:56:56.274365Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(np.floor(X_train[:, 0].min()), np.ceil(X_train[:, 0].max()), 100), np.linspace(np.floor(X_train[:, 1].min()), np.ceil(X_train[:, 1].max()), 100))\n",
    "Z = dtree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.contourf(xx, yy, Z, alpha=0.5, cmap='coolwarm_r')\n",
    "ax.set_title(\"Decision Tree\", fontsize = 20, fontweight = \"bold\", pad=15)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "plt.show()\n",
    "\n",
    "dtree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dee716",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e613f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T10:57:02.355796Z",
     "start_time": "2023-10-11T10:57:02.206092Z"
    }
   },
   "outputs": [],
   "source": [
    "rfmodel = RandomForestClassifier(max_features=\"sqrt\", max_samples=0.5, n_estimators=21)\n",
    "rfmodel.fit(X_train, y_train)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(np.floor(X_train[:, 0].min()), np.ceil(X_train[:, 0].max()), 100), np.linspace(np.floor(X_train[:, 1].min()), np.ceil(X_train[:, 1].max()), 100))\n",
    "Z = rfmodel.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.contourf(xx, yy, Z, alpha=0.5, cmap='coolwarm_r')\n",
    "ax.set_title(\"Random Forest\", fontsize = 20, fontweight = \"bold\", pad=15)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "plt.show()\n",
    "\n",
    "rfmodel.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d28b031",
   "metadata": {},
   "source": [
    "### Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ecf2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T10:57:04.907421Z",
     "start_time": "2023-10-11T10:57:04.888790Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "df_train[[\"X1\", \"X2\"]] = X_train\n",
    "df_train[\"y\"] = y_train\n",
    "\n",
    "df_test[[\"X1\", \"X2\"]] = X_test\n",
    "df_test[\"y\"] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22a0d0",
   "metadata": {},
   "source": [
    "#### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5459945",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T10:57:13.269777Z",
     "start_time": "2023-10-11T10:57:05.882205Z"
    }
   },
   "outputs": [],
   "source": [
    "total_models = 21\n",
    "row_sample_ratio, feature_sample_ratio = 0.5, 0.5\n",
    "\n",
    "all_trees = []\n",
    "\n",
    "for i in range(total_models):\n",
    "    model = DecisionTreeClassifier(max_features=\"sqrt\")\n",
    "    data = sampling_function(df_train, row_sample_ratio, feature_sample_ratio)\n",
    "    \n",
    "    model.fit(data[[\"X1\", \"X2\"]], data['y'])\n",
    "    \n",
    "    all_trees.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9a252",
   "metadata": {},
   "source": [
    "#### Aggregate predictions and create plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87c750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T10:57:13.434511Z",
     "start_time": "2023-10-11T10:57:13.272553Z"
    }
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(np.floor(X_train[:, 0].min()), np.ceil(X_train[:, 0].max()), 100), np.linspace(np.floor(X_train[:, 1].min()), np.ceil(X_train[:, 1].max()), 100))\n",
    "\n",
    "Z = predict(all_trees, np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.contourf(xx, yy, Z, alpha=0.5, cmap='coolwarm_r')\n",
    "ax.set_title(\"Custom Random Forest\", fontsize = 20, fontweight = \"bold\", pad = 15)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "plt.show()\n",
    "\n",
    "sum(predict(all_trees, df_test[[\"X1\", \"X2\"]]) == df_test.y)/df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d436ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T10:59:51.501405Z",
     "start_time": "2023-10-11T10:59:51.312170Z"
    }
   },
   "outputs": [],
   "source": [
    "dtree_accuracy = dtree.score(X_test, y_test)\n",
    "rf_accuracy = rfmodel.score(X_test, y_test)\n",
    "custom_rf_accuracy = sum(predict(all_trees, df_test[[\"X1\", \"X2\"]]) == df_test.y)/df_test.shape[0]\n",
    "\n",
    "print(f\"Decision Tree test accuracy: {dtree_accuracy}\")\n",
    "print(f\"Random Forest test accuracy: {rf_accuracy}\")\n",
    "print(f\"Custom Random Forest test accuracy: {custom_rf_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
